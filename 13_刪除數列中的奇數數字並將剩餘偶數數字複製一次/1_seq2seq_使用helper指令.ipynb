{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq\n",
    "刪除原數列中的奇數數字並將剩餘偶數數字複製一次\n",
    "<br>輸入 : [9 , 4 , 1 , 4 , 8 , 5 , 8 , 3 , 7 , 5 , 8 , 10 , 5]\n",
    "<br>輸出 : [4 , 4 , 8 , 8 , 8 , 10 , 4 , 4 , 8 , 8 , 8 , 10]\n",
    "<br><br>\n",
    "可以從結果發現在這個case中，單純只使用Seq2Seq，訓練過程是非常吃力的，所以必須加入Attention機制強化Seq2Seq，以應付較為複雜的case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "import random\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超參數\n",
    "# Number of Epochs\n",
    "epochs = 140\n",
    "# RNN Size\n",
    "rnn_hidden_unit = 50\n",
    "# Number of Layers\n",
    "num_layers = 2\n",
    "# Embedding Size\n",
    "encoding_embedding_size = 15\n",
    "decoding_embedding_size = 15\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "vocab_size_sorce = 10 + 1  # 1~10 + 0\n",
    "vocab_size_target = 10 + 3 # 1~10 + 0 & 11 & 12\n",
    "max_len = 24\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 準備數據(將數字串中的奇數刪除，並將剩下的數字再複製一遍)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "GO = 11\n",
    "EOS = 12\n",
    "odd_list, even_list = [1, 3, 5, 7, 9] * 10, [2, 4, 6, 8, 10] * 10\n",
    "\n",
    "def get_batches(num_samples = batch_size , copy_sequence = True):  \n",
    "    num_odds = np.random.randint(low = 1 , high = max_len//2 , size = num_samples)\n",
    "    num_evens = np.random.randint(low = 1 , high = max_len//2 , size = num_samples)\n",
    "    batch_len_x = num_odds + num_evens\n",
    "    if copy_sequence:\n",
    "        batch_len_y = num_evens * 2 + 1  # append <EOS> (or prepend <GO>)\n",
    "    else:\n",
    "        batch_len_y = num_evens + 1  # append <EOS> (or prepend <GO>)\n",
    "\n",
    "    batch_max_length_x = np.max(batch_len_x)\n",
    "    batch_max_length_y = np.max(batch_len_y)\n",
    "\n",
    "    batch_data_x, batch_data_y = [], []\n",
    "    for i in range(0 , num_samples):\n",
    "        odds = random.sample(odd_list , num_odds[i])\n",
    "        evens = random.sample(even_list , num_evens[i])\n",
    "        sample_x = odds + evens\n",
    "        random.shuffle(sample_x)\n",
    "\n",
    "        sample_y = list(filter(lambda x: x % 2 == 0 , sample_x))\n",
    "        if copy_sequence:\n",
    "            sample_y += sample_y\n",
    "        sample_x = np.r_[sample_x , [PAD] * (batch_max_length_x - len(sample_x))]\n",
    "        sample_y = np.r_[sample_y , [EOS] , [PAD] * (batch_max_length_y - len(sample_y) - 1)]\n",
    "\n",
    "        batch_data_x.append(sample_x)\n",
    "        batch_data_y.append(sample_y)\n",
    "\n",
    "    batch_data_x = np.array(batch_data_x , dtype = np.int32)\n",
    "    batch_data_y = np.array(batch_data_y , dtype = np.int32)\n",
    "\n",
    "    return batch_data_x , batch_data_y , batch_len_x , batch_len_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 輸入層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = tf.placeholder(tf.int32, [None , None] , name = 'inputs')\n",
    "targets = tf.placeholder(tf.int32, [None , None] , name = 'targets')\n",
    "lr = tf.placeholder(tf.float32 , name = 'learning_rate')\n",
    "\n",
    "source_sequence_length = tf.placeholder(tf.int32 , (None ,) , name = 'source_sequence_length')\n",
    "target_sequence_length = tf.placeholder(tf.int32 , (None ,) , name = 'target_sequence_length')\n",
    "\n",
    "# 決定target序列最大長度（之後target_sequence_length和source_sequence_length會作為feed_dict的參數）\n",
    "max_target_sequence_length = tf.reduce_max(target_sequence_length , name = 'max_target_len')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder embedding\n",
    "'''\n",
    "encoder_embed_input = tf.contrib.layers.embed_sequence(input_data , source_vocab_size , encoding_embedding_size) \n",
    "                                                  ⇕ 相當於\n",
    "encoder_embeddings = tf.Variable(tf.random_uniform([source_vocab_size , encoding_embedding_size]))\n",
    "encoder_embed_input = tf.nn.embedding_lookup(encoder_embeddings , input_data)\n",
    "\n",
    "若懶得寫兩行程式可以直接用tf.contrib.layers.embed_sequence這個函數\n",
    "介紹 : https://www.tensorflow.org/api_docs/python/tf/contrib/layers/embed_sequence\n",
    "'''\n",
    "encoder_embed_input = tf.contrib.layers.embed_sequence(input_data , vocab_size_sorce , encoding_embedding_size)\n",
    "\n",
    "# RNN cell\n",
    "def get_lstm_cell(rnn_hidden_unit):\n",
    "    lstm_cell = tf.contrib.rnn.LSTMCell(rnn_hidden_unit, \n",
    "                                        initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "    return lstm_cell\n",
    "\n",
    "cell = tf.contrib.rnn.MultiRNNCell([get_lstm_cell(rnn_hidden_unit) for _ in range(num_layers)])\n",
    "\n",
    "encoder_output, encoder_state = tf.nn.dynamic_rnn(cell, \n",
    "                                                  encoder_embed_input, \n",
    "                                                  sequence_length = source_sequence_length,\n",
    "                                                  dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 預處理後的decoder輸入\n",
    "# 在batch中每一筆data最前面加上GO，並移除最後一個字，所以每一筆data的詞的數目並無改變\n",
    "ending = tf.identity(targets[: , 0:-1])\n",
    "decoder_input = tf.concat([tf.fill([batch_size, 1], GO), ending], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.contrib.seq2seq.TrainingHelper:(Training 階段，還有其他種類的Helper)\n",
    "### 訓練時採用teacher forcing，永遠把ground truth輸入給模型，不管模型前一步預測結果是否正確\n",
    "此函數為Decoder端用來訓練的參數，這個函數不會把t-1階段的輸出當作t階段的輸入，而是把target中的真實質直接輸入給RNN<br>\n",
    "主要參數是inputs與sequence_length，返回helper對象，可以做為Basic Decoder函數的參數\n",
    "<br><br><br>\n",
    "\n",
    "## tf.contrib.seq2seq.GreedyEmbeddingHelper:(Inference 階段，還有其他種類的Helper)\n",
    "### 它和TrainingHelper的區別在於它會把把t-1階段的輸出進行embedding後再輸入給RNN，並且經過embedding層作為下一時刻的輸入\n",
    "• greedy decoding：每一次把模型認為概率最大的 token 輸入給下一時間步<br>\n",
    "• beam search decoding：每次保留 top k 的預測結果，解碼得到（近似） k best 序列 <br>\n",
    "• sample decoding：每一步從模型預測的概率分布裏隨機采一個 token 輸入給下一時間步"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Embedding，需要對target數據進行embedding，再傳入Decoder中的RNN\n",
    "decoder_embeddings = tf.Variable(tf.random_uniform([vocab_size_target , decoding_embedding_size]))\n",
    "decoder_embed_input = tf.nn.embedding_lookup(decoder_embeddings , decoder_input)\n",
    "\n",
    "# 2. 建造Decoder中的RNN單元\n",
    "def get_decoder_cell(rnn_hidden_unit):\n",
    "    decoder_cell = tf.contrib.rnn.LSTMCell(rnn_hidden_unit,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "    return decoder_cell\n",
    "cell = tf.contrib.rnn.MultiRNNCell([get_decoder_cell(rnn_hidden_unit) for _ in range(num_layers)])\n",
    " \n",
    "# 3. Output全連接層\n",
    "output_layer = Dense(vocab_size_target ,\n",
    "                     kernel_initializer = tf.truncated_normal_initializer(mean = 0.0 , stddev = 0.1))\n",
    "\n",
    "\n",
    "# 5. Training decoder\n",
    "with tf.variable_scope('decoder'):\n",
    "    # tf.contrib.seq2seq.TrainingHelper即是採用Teacher Forcing的方法\n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs = decoder_embed_input,\n",
    "                                                        sequence_length = target_sequence_length,\n",
    "                                                        time_major = False)\n",
    "    \n",
    "    # 構造decoder\n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(cell,\n",
    "                                                       training_helper,\n",
    "                                                       encoder_state,\n",
    "                                                       output_layer) \n",
    "    \n",
    "    training_decoder_output ,\\\n",
    "    training_final_state ,\\\n",
    "    training_final_sequence_lengths =\\\n",
    "    tf.contrib.seq2seq.dynamic_decode(training_decoder,                                          \n",
    "                                      impute_finished = True,\n",
    "                                      maximum_iterations = max_target_sequence_length)\n",
    "    \n",
    "with tf.variable_scope('decoder' , reuse = True):\n",
    "    \n",
    "    tf.get_variable_scope().reuse_variables() \n",
    "    \n",
    "    # 創建一個常量tensor並覆制為batch_size的大小\n",
    "    start_tokens = tf.tile(tf.constant([GO] , dtype=tf.int32),\n",
    "                           [batch_size] , \n",
    "                           name = 'start_tokens')\n",
    "    \n",
    "    # GreedyEmbeddingHelper採取argmax抽樣演算法來得到輸出id，並且經過embedding層作為下一時刻的輸入\n",
    "    predicting_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(decoder_embeddings,\n",
    "                                                                 start_tokens,\n",
    "                                                                 EOS)\n",
    "    \n",
    "    predicting_decoder = tf.contrib.seq2seq.BasicDecoder(cell,\n",
    "                                                         predicting_helper,\n",
    "                                                         encoder_state,\n",
    "                                                         output_layer)\n",
    "    \n",
    "    predicting_decoder_output ,\\\n",
    "    predicting_final_state ,\\\n",
    "    predicting_final_sequence_lengths =\\\n",
    "    tf.contrib.seq2seq.dynamic_decode(predicting_decoder,\n",
    "                                      impute_finished = True,\n",
    "                                      maximum_iterations = max_target_sequence_length)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_logits = tf.identity(training_decoder_output.rnn_output , 'logits')\n",
    "predicting_logits = tf.identity(predicting_decoder_output.sample_id , name='predictions')\n",
    "\n",
    "masks = tf.sequence_mask(target_sequence_length , \n",
    "                         max_target_sequence_length, \n",
    "                         dtype = tf.float32, \n",
    "                         name = 'masks')\n",
    "\n",
    "with tf.variable_scope('optimization'):        \n",
    "    # Loss function\n",
    "    cost = tf.contrib.seq2seq.sequence_loss(training_logits,\n",
    "                                            targets,\n",
    "                                            masks)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch : 99 \n",
      "Training Loss : 1.883\n",
      "New Record!\n",
      "Source : [ 9  4  1  4  8  5  8  3  7  5  8 10  5  0  0  0  0  0  0  0]\n",
      "Target : [ 4  4  8  8  8 10  4  4  8  8  8 10 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Predict: [ 6  6  8  8  8  8  8  8  8  8  8  8  8  8 12  0  0  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 199 \n",
      "Training Loss : 1.648\n",
      "New Record!\n",
      "Source : [10  1  1  1  7  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Target : [10 10 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Predict: [ 4  4 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 299 \n",
      "Training Loss : 1.548\n",
      "New Record!\n",
      "Source : [ 4  3 10  9  8  7  3  5  8  6  3  2  7  3  2  6 10  0  0  0  0  0]\n",
      "Target : [ 4 10  8  8  6  2  2  6 10  4 10  8  8  6  2  2  6 10 12  0  0  0  0]\n",
      "Predict: [ 6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6 12  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 399 \n",
      "Training Loss : 1.517\n",
      "New Record!\n",
      "Source : [ 8  2  4  5  4  9  6  9  9  5  3 10  1 10  3  7  5  2  4 10  3  0]\n",
      "Target : [ 8  2  4  4  6 10 10  2  4 10  8  2  4  4  6 10 10  2  4 10 12  0  0]\n",
      "Predict: [ 8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  4 12  0  0]\n",
      "\n",
      "\n",
      "Batch : 499 \n",
      "Training Loss : 1.502\n",
      "No Improvement.\n",
      "Source : [ 4  6  9  7  1  2  4  8  4  6  6  5  3 10  5  1  2  0  0  0  0  0]\n",
      "Target : [ 4  6  2  4  8  4  6  6 10  2  4  6  2  4  8  4  6  6 10  2 12  0  0]\n",
      "Predict: [ 8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8 12  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 599 \n",
      "Training Loss : 1.512\n",
      "New Record!\n",
      "Source : [ 2  4  3 10  5  8  8  3  7  2 10  5  2  7  0  0  0  0  0  0  0  0]\n",
      "Target : [ 2  4 10  8  8  2 10  2  2  4 10  8  8  2 10  2 12  0  0  0  0  0  0]\n",
      "Predict: [10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 12  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 699 \n",
      "Training Loss : 1.471\n",
      "New Record!\n",
      "Source : [ 5  9 10  8  5  3  8  6  6  3  8 10  4  3  9  0  0  0  0  0  0]\n",
      "Target : [10  8  8  6  6  8 10  4 10  8  8  6  6  8 10  4 12  0  0  0  0  0  0]\n",
      "Predict: [ 8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8 12  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 799 \n",
      "Training Loss : 1.421\n",
      "New Record!\n",
      "Source : [ 8  3  6  8  2 10  8 10  4  9  8  2  2  0  0  0  0  0  0  0  0]\n",
      "Target : [ 8  6  8  2 10  8 10  4  8  2  2  8  6  8  2 10  8 10  4  8  2  2 12]\n",
      "Predict: [ 8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8 12]\n",
      "\n",
      "\n",
      "Batch : 899 \n",
      "Training Loss : 1.375\n",
      "No Improvement.\n",
      "Source : [ 3  4  1  8  8 10  1 10  5  8  4  1  2  6  6  9  5  0  0  0  0]\n",
      "Target : [ 4  8  8 10 10  8  4  2  6  6  4  8  8 10 10  8  4  2  6  6 12  0  0]\n",
      "Predict: [ 8  8  8  8  8  4  4  4  4  4  6  6  6  6  6  4  4  4  6  6 12  0  0]\n",
      "\n",
      "\n",
      "Batch : 999 \n",
      "Training Loss : 1.333\n",
      "New Record!\n",
      "Source : [ 9  1  9  1  1  1  9  6  2  6  8 10  7  2  8  5  6  9  4  2  0]\n",
      "Target : [ 6  2  6  8 10  2  8  6  4  2  6  2  6  8 10  2  8  6  4  2 12  0  0]\n",
      "Predict: [ 6  6  6  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  4  4 12  0  0]\n",
      "\n",
      "\n",
      "Batch : 1099 \n",
      "Training Loss : 1.269\n",
      "New Record!\n",
      "Source : [7 9 5 2 5 5 3 7 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Target : [ 2  2 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Predict: [ 2  2 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 1199 \n",
      "Training Loss : 1.201\n",
      "New Record!\n",
      "Source : [ 7  3  7  9  8  9  5 10  2 10  0  0  0  0  0  0  0  0  0  0]\n",
      "Target : [ 8 10  2 10  8 10  2 10 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Predict: [10 10 10 10  2  2 10 10 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 1299 \n",
      "Training Loss : 1.099\n",
      "New Record!\n",
      "Source : [ 1  1  3  4  7  8  3  1 10  3  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Target : [ 4  8 10  4  8 10 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Predict: [ 8  8  8  8  4 10 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 1399 \n",
      "Training Loss : 1.007\n",
      "No Improvement.\n",
      "Source : [ 9  4  4  1  5  1  4  3  7 10 10  7 10  7  3  9  3 10  8  2  0  0]\n",
      "Target : [ 4  4  4 10 10 10 10  8  2  4  4  4 10 10 10 10  8  2 12  0  0  0  0]\n",
      "Predict: [ 4  4 10 10 10  2  4 10  8  8  2 10  8  8  2 10 10 10 12  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 1499 \n",
      "Training Loss : 0.922\n",
      "New Record!\n",
      "Source : [ 3  1  7  4  5  2  4 10  3  3  1  1  6  8  8  7 10  9  0  0  0  0]\n",
      "Target : [ 4  2  4 10  6  8  8 10  4  2  4 10  6  8  8 10 12  0  0  0  0  0  0]\n",
      "Predict: [ 4  4  2 10  8  8  4  6  2  8  8 10  4  6 10  8 12  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 1599 \n",
      "Training Loss : 0.838\n",
      "New Record!\n",
      "Source : [ 4  6  1  3  3  5  4  7 10  6 10  6 10  3  8  0  0  0  0  0  0  0]\n",
      "Target : [ 4  6  4 10  6 10  6 10  8  4  6  4 10  6 10  6 10  8 12  0  0  0  0]\n",
      "Predict: [ 4  6  6  8 10 10  6  8  6  8  6  8  2  8  6 10  8  6 12  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 1699 \n",
      "Training Loss : 0.768\n",
      "New Record!\n",
      "Source : [2 4 3 6 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Target : [ 2  4  6  2  4  6 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Predict: [ 2  4  6  2  6  4 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 1799 \n",
      "Training Loss : 0.699\n",
      "New Record!\n",
      "Source : [ 7  4 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Target : [ 4 10  4 10 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Predict: [ 4 10 10  4 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 1899 \n",
      "Training Loss : 0.656\n",
      "No Improvement.\n",
      "Source : [4 7 8 9 9 5 2 5 2 2 8 3 6 3 5 4 1 4 3 0 0]\n",
      "Target : [ 4  8  2  2  2  8  6  4  4  4  8  2  2  2  8  6  4  4 12  0  0  0  0]\n",
      "Predict: [ 4 10  2  4  4  6  2  4 10  4  4  2  4  2  6  8  6  2 12  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 1999 \n",
      "Training Loss : 0.610\n",
      "New Record!\n",
      "Source : [ 3  2  8  5  7 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Target : [ 2  8 10  2  8 10 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Predict: [ 2 10  8  2  8 10 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 2099 \n",
      "Training Loss : 0.572\n",
      "New Record!\n",
      "Source : [ 5  2  2  2  4  1  6  5  6  8  6  4  5 10  4  0  0  0  0  0  0]\n",
      "Target : [ 2  2  2  4  6  6  8  6  4 10  4  2  2  2  4  6  6  8  6  4 10  4 12]\n",
      "Predict: [ 2  2  2  6  4  4  4  6 10  2 10  6  4  6  4  2  6  4  8 10  2  6 12]\n",
      "\n",
      "\n",
      "Batch : 2199 \n",
      "Training Loss : 0.554\n",
      "New Record!\n",
      "Source : [ 5  3  3  2  8  5  1  7  9  8 10  4  2  7  0  0  0  0  0  0  0]\n",
      "Target : [ 2  8  8 10  4  2  2  8  8 10  4  2 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Predict: [ 2  8  8  2  8  2  8  2  8  2  8  8 12  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 2299 \n",
      "Training Loss : 0.514\n",
      "New Record!\n",
      "Source : [1 7 8 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Target : [ 8  8 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Predict: [ 8  8 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 2399 \n",
      "Training Loss : 0.487\n",
      "New Record!\n",
      "Source : [2 5 5 4 8 9 6 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Target : [ 2  4  8  6  2  4  8  6 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Predict: [ 2  4  8  6  2  4  8  6 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 2499 \n",
      "Training Loss : 0.466\n",
      "New Record!\n",
      "Source : [ 6 10  9  6  8  4  8  7  2  2  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Target : [ 6 10  6  8  4  8  2  2  6 10  6  8  4  8  2  2 12  0  0  0  0  0  0]\n",
      "Predict: [ 6 10  4  8  6 10  2  6  4  8 10  6  2  8  4  2 12  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 2599 \n",
      "Training Loss : 0.441\n",
      "No Improvement.\n",
      "Source : [8 5 8 2 7 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Target : [ 8  8  2  6  8  8  2  6 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Predict: [ 8  8  6  2  8  8  2  6 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 2699 \n",
      "Training Loss : 0.409\n",
      "New Record!\n",
      "Source : [ 9  3  6  3  3  9 10  1  3  7  7 10  0  0  0  0  0  0  0]\n",
      "Target : [ 6 10 10  6 10 10 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Predict: [ 6 10 10  6 10 10 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 2799 \n",
      "Training Loss : 0.407\n",
      "No Improvement.\n",
      "Source : [ 2  4 10 10  8  6  4  8  8  8  9  6  0  0  0  0  0  0  0  0]\n",
      "Target : [ 2  4 10 10  8  6  4  8  8  8  6  2  4 10 10  8  6  4  8  8  8  6 12]\n",
      "Predict: [ 2  2  8  8  8  6 10  4  8  2  6  8  4 10  8  6  8 10  4  4  8 10 12]\n",
      "\n",
      "\n",
      "Batch : 2899 \n",
      "Training Loss : 0.370\n",
      "New Record!\n",
      "Source : [1 8 7 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Target : [ 8  8 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Predict: [ 8  8 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 2999 \n",
      "Training Loss : 0.353\n",
      "No Improvement.\n",
      "Source : [7 1 6 3 4 3 8 4 5 4 7 7 7 9 1 0 0 0 0 0]\n",
      "Target : [ 6  4  8  4  4  6  4  8  4  4 12  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Predict: [ 6  4  8  4  4  6  4  8  4  4 12  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch : 3099 \n",
      "Training Loss : 0.349\n",
      "No Improvement.\n",
      "Source : [ 3 10  9  5 10  8  2 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Target : [10 10  8  2 10 10 10  8  2 10 12  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Predict: [10 10 10  4 10 10 10 10  2  8 12  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 3199 \n",
      "Training Loss : 0.330\n",
      "New Record!\n",
      "Source : [ 9  2  6 10  1  2  3  3  1  6  4 10  1 10  1  9  5  0  0  0  0]\n",
      "Target : [ 2  6 10  2  6  4 10 10  2  6 10  2  6  4 10 10 12  0  0  0  0  0  0]\n",
      "Predict: [ 2  6  2 10  6  8  2 10  2  6  2 10  6  8  2 10 12  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 3299 \n",
      "Training Loss : 0.315\n",
      "No Improvement.\n",
      "Source : [ 8  6  9  3  8  3 10  2  7  4  8 10 10  0  0  0  0  0  0  0  0]\n",
      "Target : [ 8  6  8 10  2  4  8 10 10  8  6  8 10  2  4  8 10 10 12  0  0  0  0]\n",
      "Predict: [ 8  6  8  2 10  8 10  4 10  8  6  8 10  2 10  8  4 10 12  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 3399 \n",
      "Training Loss : 0.293\n",
      "No Improvement.\n",
      "Source : [ 1  7  9  1 10  6  4  2  2  6  4  4  6  0  0  0  0  0  0  0  0]\n",
      "Target : [10  6  4  2  2  6  4  4  6 10  6  4  2  2  6  4  4  6 12  0  0  0  0]\n",
      "Predict: [10  6  2  4  4  6  2  6 10  4  6  2  4  4  6  2  6  8 12  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 3499 \n",
      "Training Loss : 0.283\n",
      "New Record!\n",
      "Source : [ 9  6 10  1  5  7  3  4  2  3  4 10  7  0  0  0  0  0  0  0  0  0]\n",
      "Target : [ 6 10  4  2  4 10  6 10  4  2  4 10 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Predict: [ 6 10  4  2  4 10  6 10  4  2  4 10 12  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 3599 \n",
      "Training Loss : 0.295\n",
      "New Record!\n",
      "Source : [2 9 9 5 7 8 7 3 1 3 5 6 4 3 0 0 0 0 0 0 0]\n",
      "Target : [ 2  8  6  4  2  8  6  4 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Predict: [ 2  8  6  4  2  8  6  4 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 3699 \n",
      "Training Loss : 0.269\n",
      "No Improvement.\n",
      "Source : [8 5 8 5 3 9 7 5 3 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Target : [ 8  8  8  8 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Predict: [ 8  8  8  8 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 3799 \n",
      "Training Loss : 0.261\n",
      "New Record!\n",
      "Source : [ 3  6  1  1  7  9  1 10  5 10  9  5  3  0  0  0  0  0  0  0  0  0]\n",
      "Target : [ 6 10 10  6 10 10 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Predict: [ 6 10 10  6 10 10 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 3899 \n",
      "Training Loss : 0.246\n",
      "No Improvement.\n",
      "Source : [ 3  1  7  2  8 10 10  9  5  8  7  9  1  7  0  0  0  0  0]\n",
      "Target : [ 2  8 10 10  8  2  8 10 10  8 12  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Predict: [ 2  8 10 10  8  2  8 10 10  8 12  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 3999 \n",
      "Training Loss : 0.253\n",
      "No Improvement.\n",
      "Source : [9 2 8 9 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Target : [ 2  8  2  2  8  2 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Predict: [ 2  8  2  2  8  2 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 4099 \n",
      "Training Loss : 0.239\n",
      "New Record!\n",
      "Source : [9 2 7 9 5 3 2 5 6 8 5 0 0 0 0 0 0 0 0 0 0]\n",
      "Target : [ 2  2  6  8  2  2  6  8 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Predict: [ 2  2  6  8  2  2  6  8 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 4199 \n",
      "Training Loss : 0.220\n",
      "New Record!\n",
      "Source : [ 3 10  1  1  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Target : [10  8 10  8 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Predict: [10  8 10  8 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 4299 \n",
      "Training Loss : 0.210\n",
      "New Record!\n",
      "Source : [10 10  6 10 10  2  9  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Target : [10 10  6 10 10  2  8 10 10  6 10 10  2  8 12  0  0  0  0  0  0  0  0]\n",
      "Predict: [10 10  6 10 10  2  8 10 10  6 10 10  2  8 12  0  0  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 4399 \n",
      "Training Loss : 0.212\n",
      "New Record!\n",
      "Source : [9 9 1 5 4 6 3 2 8 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Target : [ 4  6  2  8  4  6  2  8 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Predict: [ 4  6  2  8  4  6  2  8 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 4499 \n",
      "Training Loss : 0.202\n",
      "New Record!\n",
      "Source : [10 10  7  6  1  2  7  8  6  7 10  9  6  7  0  0  0  0  0  0  0  0]\n",
      "Target : [10 10  6  2  8  6 10  6 10 10  6  2  8  6 10  6 12  0  0  0  0  0  0]\n",
      "Predict: [10 10  6  6 10  4 10  6 10  6 10  6 10  4  6 10 12  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 4599 \n",
      "Training Loss : 0.193\n",
      "New Record!\n",
      "Source : [ 8  2  1 10  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Target : [ 8  2 10  2  8  2 10  2 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Predict: [ 8  2 10  2  8  2 10  2 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 4699 \n",
      "Training Loss : 0.196\n",
      "New Record!\n",
      "Source : [ 7  7 10  7  4  3  6  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Target : [10  4  6 10  4  6 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Predict: [10  4  6 10  4  6 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 4799 \n",
      "Training Loss : 0.174\n",
      "No Improvement.\n",
      "Source : [ 8  3  1  7 10  8  9 10  6  7  4  2  9  0  0  0  0  0  0  0  0  0]\n",
      "Target : [ 8 10  8 10  6  4  2  8 10  8 10  6  4  2 12  0  0  0  0  0  0  0  0]\n",
      "Predict: [ 8 10  8 10  6  4  2  8 10  8 10  6  4  2 12  0  0  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 4899 \n",
      "Training Loss : 0.177\n",
      "No Improvement.\n",
      "Source : [1 8 2 7 2 5 3 9 7 4 2 8 7 0 0 0 0 0 0 0 0 0]\n",
      "Target : [ 8  2  2  4  2  8  8  2  2  4  2  8 12  0  0  0  0  0  0  0  0  0  0]\n",
      "Predict: [ 8  2  2  4  2  8  8  2  2  4  2  8 12  0  0  0  0  0  0  0  0  0  0]\n",
      "\n",
      "\n",
      "Batch : 4999 \n",
      "Training Loss : 0.168\n",
      "No Improvement.\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "loss_ = []\n",
    "stop_early = 0\n",
    "for batch_i in range(0 , 5000):\n",
    "    \n",
    "    # 在每進行一個epoch前，把每個batch的index先決定出來\n",
    "    pad_train_source_batch , pad_train_target_batch, train_source_length , train_target_length = get_batches()\n",
    "\n",
    "        \n",
    "    _ , loss , predicting_logits_result =\\\n",
    "    sess.run([train_op, cost , predicting_logits], \n",
    "             feed_dict = {input_data : pad_train_source_batch ,\n",
    "                          targets : pad_train_target_batch ,\n",
    "                          source_sequence_length: train_source_length , \n",
    "                          target_sequence_length : train_target_length  ,   \n",
    "                          lr: learning_rate})\n",
    "    \n",
    "    loss_.append(loss)    \n",
    "    if len(loss_) == 100:\n",
    "        loss_ = np.array(loss_)\n",
    "        print('\\nBatch : {} \\nTraining Loss : {:.3f}'.format(batch_i , loss_.mean()))\n",
    "        if loss <= loss_.mean():\n",
    "            print('New Record!') \n",
    "            stop_early = 0\n",
    "        else:\n",
    "            print('No Improvement.')\n",
    "            stop_early += 1\n",
    "            if stop_early == 3:\n",
    "                break            \n",
    "        loss_ = []\n",
    "        \n",
    "        print('Source : {}'.format(pad_train_source_batch[0 , :]))\n",
    "        print('Target : {}'.format(pad_train_target_batch[0 , :]))\n",
    "        print('Predict: {}\\n'.format(predicting_logits_result[0 , :]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
