{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq_Attension\n",
    "實現機器翻譯(Machine Translation)，輸入一句英文句子，輸出一句法文句子\n",
    "<br>輸入 : his least liked fruit is the apple , but your least liked is the strawberry .\n",
    "<br>輸出 : son fruit est moins aimé la pomme , mais votre moins aimé est la fraise .\n",
    "<br>在這支程式中加入Attension機制，強化Seq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在一般的Seq2Seq模型的inference階段中，如果Sequence中在t時刻中產生錯誤的值，在t時刻之後的輸入狀態將會受到影響，而該誤差會隨著生成過程不斷向後累積；而Scheduled Sampling以一定概率將Decoder自己產生的值作為Decoder端的輸入，這樣即使前面產生錯誤的值，其目標仍然是最大化真實目標序列的概率，模型會朝著正確的方向進<br>\n",
    "\n",
    "在訓練早期Scheduled Sampling主要使用target中的真實值作為Decoder端的輸入，可以將模型從隨機初始化的狀態快速引導至一個合理的狀態；隨著訓練的進行，該方法會逐漸更多地使用Decoder自己產生的值作為Decoder端的輸入，以解決數據分布不一致的問題<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import copy\n",
    "import random\n",
    "from tensorflow.python.layers.core import Dense\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 讀取數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English source data\n",
    "f_eng = open('data/small_vocab_en' , 'r' , encoding = 'utf-8')\n",
    "source_letter = []\n",
    "source_sentence = []\n",
    "while True:\n",
    "    raw = f_eng.readline()\n",
    "    if raw == '' : break\n",
    "\n",
    "    sentence = raw.split('\\n')[0] \n",
    "    temp_sentence = []\n",
    "    for word in sentence.split(' '):\n",
    "        if len(word) != 0:\n",
    "            source_letter.append(word.lower())\n",
    "            temp_sentence.append(word.lower())\n",
    "    source_sentence.append(temp_sentence)    \n",
    "\n",
    "    \n",
    "# French target data\n",
    "f_fre = open('data/small_vocab_fr', 'r', encoding='utf-8')       \n",
    "target_letter = []\n",
    "target_sentence = []\n",
    "while True:\n",
    "    raw = f_fre.readline()\n",
    "    if raw == '' : break\n",
    "\n",
    "    sentence = raw.split('\\n')[0]   \n",
    "    temp_sentence = []\n",
    "    for word in sentence.split(' '):\n",
    "        if len(word) != 0:\n",
    "            target_letter.append(word.lower())\n",
    "            temp_sentence.append(word.lower())\n",
    "    target_sentence.append(temp_sentence)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 數據預處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_words = ['<PAD>' , '<UNK>' , '<GO>' , '<EOS>']\n",
    "\n",
    "# 建造英文詞庫\n",
    "source_letter = list(set(source_letter)) + special_words[:2] # 加入 '<PAD>' , '<UNK>'              \n",
    "source_letter_to_int = {word : idx for idx , word in enumerate(source_letter)}   \n",
    "source_int_to_letter = {idx : word for idx , word in enumerate(source_letter)}   \n",
    "\n",
    "# 建造法文詞庫\n",
    "target_letter = list(set(target_letter)) + special_words # 加入 '<PAD>' , '<UNK>' , '<GO>' , '<EOS>'       \n",
    "target_letter_to_int = {word : idx for idx , word in enumerate(target_letter)}   \n",
    "target_int_to_letter = {idx : word for idx , word in enumerate(target_letter)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將所有字母轉換成index\n",
    "source_int = []\n",
    "for sentence in source_sentence:\n",
    "    temp = []\n",
    "    for letter in sentence:\n",
    "        if letter in source_letter_to_int.keys():\n",
    "            temp.append(source_letter_to_int[letter])  \n",
    "        else:\n",
    "            temp.append(source_letter_to_int['<UNK>'])\n",
    "    source_int.append(temp)           \n",
    "            \n",
    "target_int = []\n",
    "for sentence in target_sentence:\n",
    "    temp = []\n",
    "    for letter in sentence:\n",
    "        if letter in target_letter_to_int.keys():\n",
    "            temp.append(target_letter_to_int[letter])\n",
    "        else:\n",
    "            temp.append(target_letter_to_int['<UNK>'])\n",
    "    temp.append(target_letter_to_int['<EOS>'])          \n",
    "    target_int.append(temp)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超參數\n",
    "# Number of Epochs\n",
    "epochs = 200\n",
    "# Batch Size\n",
    "batch_size = 130\n",
    "# RNN Size\n",
    "rnn_hidden_unit = 128\n",
    "# Number of Layers\n",
    "num_layers = 1\n",
    "# Embedding Size\n",
    "encoding_embedding_size = 100\n",
    "decoding_embedding_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 輸入層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = tf.placeholder(tf.int32 , [None , None] , name = 'inputs')\n",
    "targets = tf.placeholder(tf.int32 , [None , None] , name = 'targets')\n",
    "from_model_or_target = tf.placeholder(tf.float32 , [] , name = 'from_model_or_target')\n",
    "\n",
    "source_sequence_length = tf.placeholder(tf.int32 , [None ,] , name = 'source_sequence_length')\n",
    "target_sequence_length = tf.placeholder(tf.int32 , [None ,] , name = 'target_sequence_length')\n",
    "# 決定target序列最大長度（之後target_sequence_length和source_sequence_length會作為feed_dict的參數）\n",
    "max_target_sequence_length = tf.reduce_max(target_sequence_length , name = 'max_target_len')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要對source數據進行embedding，再傳入Decoder中的RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_data: 輸入tensor\n",
    "# rnn_hidden_unit: rnn隱層結點數量\n",
    "# num_layers: rnn cell的層數\n",
    "# source_sequence_length: source數據的序列長度\n",
    "# source_vocab_size: source數據的詞庫大小\n",
    "# encoding_embedding_size: embedding的向量維度\n",
    "\n",
    "# Encoder embedding\n",
    "'''\n",
    "encoder_embed_input = tf.contrib.layers.embed_sequence(input_data , source_vocab_size , encoding_embedding_size) \n",
    "                                                  ⇕ 相當於\n",
    "encoder_embeddings = tf.Variable(tf.random_uniform([source_vocab_size , encoding_embedding_size]))\n",
    "encoder_embed_input = tf.nn.embedding_lookup(encoder_embeddings , input_data)\n",
    "\n",
    "若懶得寫兩行程式可以直接用tf.contrib.layers.embed_sequence這個函數\n",
    "介紹 : https://www.tensorflow.org/api_docs/python/tf/contrib/layers/embed_sequence\n",
    "'''\n",
    "source_vocab_size = len(source_letter_to_int)\n",
    "encoder_embeddings = tf.Variable(tf.random_uniform([source_vocab_size , encoding_embedding_size]))\n",
    "encoder_embed_input = tf.nn.embedding_lookup(encoder_embeddings , input_data)\n",
    "\n",
    "def get_lstm_cell(rnn_hidden_unit):\n",
    "    lstm_cell = tf.contrib.rnn.LSTMCell(rnn_hidden_unit, \n",
    "                                        initializer = tf.random_uniform_initializer(-0.1 , 0.1))\n",
    "    return lstm_cell\n",
    "\n",
    "cell = tf.contrib.rnn.MultiRNNCell([get_lstm_cell(rnn_hidden_unit) for _ in range(num_layers)])\n",
    "\n",
    "encoder_output, encoder_state = tf.nn.dynamic_rnn(cell, \n",
    "                                                  encoder_embed_input, \n",
    "                                                  sequence_length = source_sequence_length,\n",
    "                                                  dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder and Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 預處理後的decoder輸入\n",
    "# 在batch中每一筆data最前面加上<GO>，並移除最後一個字，所以每一筆data的詞的數目並無改變\n",
    "# cut掉最後一個字\n",
    "# ending = tf.strided_slice(targets , [0, 0] , [batch_size, -1] , [1, 1]) # 等同於 ending = tf.identity(targets[: , 0:-1])\n",
    "ending = tf.identity(targets[: , 0:-1])\n",
    "decoder_input = tf.concat([tf.fill([batch_size, 1] , target_letter_to_int['<GO>']) , ending] , axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.contrib.seq2seq.ScheduledEmbeddingTrainingHelper:(Training 階段，還有其他種類的Helper)\n",
    "\n",
    "train-decoder不再一直都是真實的lable數據作為下一個時刻的輸入<br>\n",
    "train-decoder會以一個機率P選擇模型自身的輸出作為下一個預測的輸入，以1-p選擇真實標記作為下一個預測的輸入<br>\n",
    "scheduled sampling，即機率P在訓練的過程中是變化的<br>\n",
    "一開始訓練不充分，先讓P小一些，盡量使用真實的label作為輸入，隨著訓練的進行，將P增大，採用自身的輸出作為下一個預測的輸入<br>\n",
    "隨著訓練的進行，P越來越大，train-decoder模型最終變來和inference-decoder預測模型一樣，消除了train-decoder與inference-decoder之間的差異\n",
    "<br><br><br>\n",
    "\n",
    "### tf.contrib.seq2seq.GreedyEmbeddingHelper:(Inference 階段，還有不同sample手段的Helper)\n",
    "### 它和TrainingHelper的區別在於它會把t-1時刻的輸出經過embedding層作為t時刻的輸入\n",
    "• greedy decoding：每一次把模型認為機率最大的 token 輸入給下一時刻<br>\n",
    "• beam search decoding：每次保留 top k 的預測結果，解碼得到（近似） k best 序列 <br>\n",
    "• sample decoding：每一步從模型預測的機率分布中隨機取樣一個 token 輸入給下一時刻"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoding_embedding_size: embedding的向量維度\n",
    "# num_layers: rnn cell的層數\n",
    "# rnn_size: RNN單元的隱層結點數量\n",
    "# target_sequence_length: target數據序列長度\n",
    "# max_target_sequence_length: target數據序列最大長度\n",
    "# encoder_state: encoder端編碼的狀態向量\n",
    "# decoder_input: decoder端輸入\n",
    "\n",
    "# 1. Embedding，需要對target數據進行embedding，再傳入Decoder中的RNN\n",
    "target_vocab_size = len(target_letter_to_int)\n",
    "decoder_embeddings = tf.Variable(tf.random_uniform([target_vocab_size , decoding_embedding_size]))\n",
    "decoder_embed_input = tf.nn.embedding_lookup(decoder_embeddings , decoder_input)\n",
    "\n",
    "# 2. 建造Decoder中的RNN單元\n",
    "def get_decoder_cell(rnn_hidden_unit):\n",
    "    decoder_cell = tf.contrib.rnn.LSTMCell(rnn_hidden_unit,\n",
    "                                           initializer = tf.random_uniform_initializer(-0.1 , 0.1))\n",
    "    return decoder_cell\n",
    "cell = tf.contrib.rnn.MultiRNNCell([get_decoder_cell(rnn_hidden_unit) for _ in range(num_layers)])\n",
    " \n",
    "# 3. Output全連接層\n",
    "output_layer = Dense(target_vocab_size,\n",
    "                     kernel_initializer = tf.truncated_normal_initializer(mean = 0.0 , stddev = 0.1))\n",
    " \n",
    "# 4. 加入Attention機制\n",
    "attn_mech = tf.contrib.seq2seq.LuongAttention(num_units = rnn_hidden_unit ,\n",
    "                                              memory = encoder_output ,\n",
    "                                              memory_sequence_length = source_sequence_length)\n",
    "\n",
    "attn_decoder = tf.contrib.seq2seq.AttentionWrapper(cell = cell ,\n",
    "                                                   attention_mechanism = attn_mech , \n",
    "                                                   attention_layer_size = rnn_hidden_unit , \n",
    "                                                   alignment_history = True)\n",
    " \n",
    "initial_state = attn_decoder.zero_state(batch_size , tf.float32).clone(cell_state = encoder_state)\n",
    "\n",
    "\n",
    "# 5. Training decoder\n",
    "with tf.variable_scope('decoder'):\n",
    "    # tf.contrib.seq2seq.ScheduledEmbeddingTrainingHelper即是採用scheduled sampling的方法\n",
    "    training_helper =\\\n",
    "    tf.contrib.seq2seq.ScheduledEmbeddingTrainingHelper(inputs = decoder_embed_input ,\n",
    "                                                        sequence_length = target_sequence_length,\n",
    "                                                        embedding = decoder_embeddings ,\n",
    "                                                        sampling_probability = from_model_or_target ,\n",
    "                                                        time_major = False)\n",
    "\n",
    "    # 建造decoder\n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(attn_decoder ,\n",
    "                                                       training_helper ,\n",
    "                                                       initial_state ,\n",
    "                                                       output_layer) \n",
    "    \n",
    "    # decoder_output包含 rnn_output 與 sample_id\n",
    "    # rnn_output: [batch_size, decoder_targets_length, vocab_size]，保存decode每個時刻每個單詞的概率，可以用來計算loss\n",
    "    # sample_id: [batch_size], tf.int32，保存最終的編碼結果。可以表示最後的答案\n",
    "    training_decoder_output ,\\\n",
    "    training_final_state ,\\\n",
    "    training_final_sequence_lengths =\\\n",
    "    tf.contrib.seq2seq.dynamic_decode(training_decoder,                                          \n",
    "                                      impute_finished = True,\n",
    "                                      maximum_iterations = max_target_sequence_length)\n",
    "    \n",
    "    training_logits = tf.identity(training_decoder_output.rnn_output ,  name = 'logits')\n",
    "    \n",
    "    attention_matrices = training_final_state.alignment_history.stack(name = 'train_attention_matrix')\n",
    "\n",
    "    \n",
    "with tf.variable_scope('decoder'):\n",
    "    \n",
    "    tf.get_variable_scope().reuse_variables() \n",
    "    \n",
    "    # 創建一個常量tensor並覆制為batch_size的大小\n",
    "    start_tokens = tf.tile(tf.constant([target_letter_to_int['<GO>']], dtype=tf.int32) ,\n",
    "                           [batch_size] , \n",
    "                           name = 'start_tokens')\n",
    "    \n",
    "    # GreedyEmbeddingHelper採取argmax抽樣演算法來得到輸出id，並且經過embedding層作為下一時刻的輸入\n",
    "    predicting_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(decoder_embeddings ,\n",
    "                                                                 start_tokens ,\n",
    "                                                                 target_letter_to_int['<EOS>'])\n",
    "    \n",
    "    predicting_decoder = tf.contrib.seq2seq.BasicDecoder(attn_decoder ,\n",
    "                                                         predicting_helper ,\n",
    "                                                         initial_state ,\n",
    "                                                         output_layer)\n",
    "  \n",
    "    predicting_decoder_output ,\\\n",
    "    predicting_final_state ,\\\n",
    "    predicting_final_sequence_lengths=\\\n",
    "    tf.contrib.seq2seq.dynamic_decode(predicting_decoder,\n",
    "                                      impute_finished = True,\n",
    "                                      maximum_iterations = max_target_sequence_length)  \n",
    "    \n",
    "    predicting_logits = tf.identity(predicting_decoder_output.sample_id ,  name = 'predictions')\n",
    "\n",
    "    # 產生attention矩陣，有助於最後可視化結果\n",
    "    predicting_attention_matrices = predicting_final_state.alignment_history.stack(name = 'inference_attention_matrix')                                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('optimization'):   \n",
    "    '''\n",
    "    target_sequence_length : [4 , 2 , 3]\n",
    "\n",
    "    max_target_sequence_length : 8\n",
    "\n",
    "    ⟹ masks的輸出長這樣 : 1 1 1 1 0 0 0 0  (4)\n",
    "                          1 1 0 0 0 0 0 0  (2)\n",
    "                          1 1 1 0 0 0 0 0  (3)\n",
    "    ➝ 0的部分代表是補0的地方，不列入loss的計算\n",
    "    '''                 \n",
    "    masks = tf.sequence_mask(target_sequence_length , \n",
    "                             max_target_sequence_length, \n",
    "                             dtype = tf.float32, \n",
    "                             name = 'masks')\n",
    "    \n",
    "    # Loss function\n",
    "    cost = tf.contrib.seq2seq.sequence_loss(training_logits,\n",
    "                                            targets,\n",
    "                                            masks)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad , -1. , 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(source , target , index = None , on_train = False):  \n",
    "    if on_train:\n",
    "        source = np.array(source)[index]    \n",
    "        target = np.array(target)[index]   \n",
    "    else:\n",
    "        source = np.array(source)\n",
    "        target = np.array(target)\n",
    "        \n",
    "    # 決定source與target中的最大長度\n",
    "    source_max_length , target_max_length = 0 , 0  \n",
    "    for vob_source , vob_target in zip(source , target):\n",
    "        if len(vob_source) > source_max_length:\n",
    "            source_max_length = len(vob_source)    \n",
    "        if len(vob_target) > target_max_length:\n",
    "            target_max_length = len(vob_target)  \n",
    " \n",
    "    # 分別對source與target補source_letter_to_int['<PAD>']與target_letter_to_int['<PAD>']到最大長度  \n",
    "    source_pad , target_pad = [] , []\n",
    "    source_len , target_len = [] , []\n",
    "    for source_sentence , target_sentence in zip(source , target):\n",
    "        source_len.append(len(source_sentence)) # 收集source中每個sencentence的長度\n",
    "        temp_source = source_sentence.copy()\n",
    "        while len(temp_source) < source_max_length:\n",
    "            temp_source.append(source_letter_to_int['<PAD>']) \n",
    "        source_pad.append(temp_source)\n",
    "        \n",
    "        target_len.append(len(target_sentence)) # 收集target中每個sencentence的長度\n",
    "        temp_target = target_sentence.copy()\n",
    "        while len(temp_target) < target_max_length:\n",
    "            temp_target.append(target_letter_to_int['<PAD>']) \n",
    "        target_pad.append(temp_target) \n",
    "        \n",
    "    return np.array(source_pad) , np.array(target_pad) , np.array(source_len) , np.array(target_len)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將數據集分割為train和validation\n",
    "train_source = source_int[batch_size:]\n",
    "train_target = target_int[batch_size:]\n",
    "# 留出一個batch進行驗證\n",
    "valid_source = source_int[:batch_size]\n",
    "valid_target = target_int[:batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Epoch : 0/200 \n",
      "stop_early : 0 \n",
      "Training Loss : 0.162\n",
      "******************************\n",
      "Source  : she dislikes that little red truck .\n",
      "Target  : elle déteste ce petit camion rouge .\n",
      "Predict : elle déteste ce camion bleu rouillé .\n",
      "\n",
      "==============================\n",
      "Epoch : 1/200 \n",
      "stop_early : 0 \n",
      "Training Loss : 0.063\n",
      "******************************\n",
      "Source  : the grapefruit is her least favorite fruit , but the banana is their least favorite .\n",
      "Target  : le pamplemousse est son fruit préféré moins , mais la banane est leur moins préférée .\n",
      "Predict : le pamplemousse est son fruit préféré moins , mais la banane est leur moins préférée .\n",
      "\n",
      "==============================\n",
      "Epoch : 2/200 \n",
      "stop_early : 0 \n",
      "Training Loss : 0.045\n",
      "******************************\n",
      "Source  : the lime is her least liked fruit , but the banana is my least liked .\n",
      "Target  : la chaux est son moins aimé des fruits , mais la banane est mon moins aimé.\n",
      "Predict : la chaux est son fruit moins aimé , mais la banane est mon moins aimé.\n",
      "\n",
      "==============================\n",
      "Epoch : 3/200 \n",
      "stop_early : 0 \n",
      "Training Loss : 0.042\n",
      "******************************\n",
      "Source  : i like grapes , pears , and strawberries .\n",
      "Target  : j'aime les raisins , les poires et les fraises .\n",
      "Predict : j'aime les raisins , les poires et les fraises .\n",
      "\n",
      "==============================\n",
      "Epoch : 4/200 \n",
      "stop_early : 0 \n",
      "Training Loss : 0.036\n",
      "******************************\n",
      "Source  : new jersey is sometimes hot during march , and it is beautiful in fall .\n",
      "Target  : new jersey est parfois chaud en mars , et il est beau à l' automne .\n",
      "Predict : new jersey est parfois chaud en mars , et il est beau à l' automne .\n",
      "\n",
      "==============================\n",
      "Epoch : 5/200 \n",
      "stop_early : 0 \n",
      "Training Loss : 0.027\n",
      "******************************\n",
      "Source  : she dislikes oranges and grapefruit .\n",
      "Target  : elle déteste les oranges et le pamplemousse .\n",
      "Predict : elle déteste les oranges et le pamplemousse .\n",
      "\n",
      "==============================\n",
      "Epoch : 6/200 \n",
      "stop_early : 0 \n",
      "Training Loss : 0.024\n",
      "******************************\n",
      "Source  : california is never busy during february , and it is usually hot in june .\n",
      "Target  : california est jamais occupé en février , et il est généralement chaud en juin .\n",
      "Predict : california est jamais occupé en février , et il est généralement chaud en juin .\n",
      "\n",
      "==============================\n",
      "Epoch : 7/200 \n",
      "stop_early : 0 \n",
      "Training Loss : 0.020\n",
      "******************************\n",
      "Source  : he saw a old yellow truck .\n",
      "Target  : il a vu un vieux camion jaune .\n",
      "Predict : il a vu un vieux camion jaune .\n",
      "\n",
      "==============================\n",
      "Epoch : 8/200 \n",
      "stop_early : 0 \n",
      "Training Loss : 0.019\n",
      "******************************\n",
      "Source  : paris is never pleasant during september , and it is beautiful in autumn .\n",
      "Target  : paris est jamais agréable en septembre , et il est beau à l' automne .\n",
      "Predict : paris est jamais agréable en septembre , et il est beau à l' automne .\n",
      "\n",
      "==============================\n",
      "Epoch : 9/200 \n",
      "stop_early : 0 \n",
      "Training Loss : 0.012\n",
      "******************************\n",
      "Source  : the orange is her least liked fruit , but the grapefruit is their least liked .\n",
      "Target  : l'orange est la moins aimé des fruits , mais le pamplemousse est leur moins aimé .\n",
      "Predict : l'orange est la moins aimé des fruits , mais le pamplemousse est leur moins aimé .\n",
      "\n",
      "==============================\n",
      "Epoch : 10/200 \n",
      "stop_early : 0 \n",
      "Training Loss : 0.016\n",
      "******************************\n",
      "Source  : paris is mild during summer , but it is usually busy in april .\n",
      "Target  : paris est doux pendant l' été , mais il est généralement occupé en avril .\n",
      "Predict : paris est doux pendant l' été , mais il est généralement occupé en avril .\n",
      "\n",
      "==============================\n",
      "Epoch : 11/200 \n",
      "stop_early : 0 \n",
      "Training Loss : 0.013\n",
      "******************************\n",
      "Source  : california is beautiful during january , and it is pleasant in february .\n",
      "Target  : californie est beau en janvier , et il est agréable en février .\n",
      "Predict : californie est beau en janvier , et il est agréable en février .\n",
      "\n",
      "==============================\n",
      "Epoch : 12/200 \n",
      "stop_early : 0 \n",
      "Training Loss : 0.012\n",
      "******************************\n",
      "Source  : he dislikes lemons , grapes , and mangoes.\n",
      "Target  : il déteste les citrons , les raisins et les mangues .\n",
      "Predict : il déteste les citrons , les raisins et les mangues .\n",
      "\n",
      "==============================\n",
      "Epoch : 13/200 \n",
      "stop_early : 0 \n",
      "Training Loss : 0.014\n",
      "******************************\n",
      "Source  : california is never cold during february , but it is sometimes freezing in june .\n",
      "Target  : californie ne fait jamais froid en février , mais il est parfois le gel en juin .\n",
      "Predict : californie ne fait jamais froid en février , mais il est parfois le gel en juin .\n",
      "\n",
      "==============================\n",
      "Epoch : 14/200 \n",
      "stop_early : 0 \n",
      "Training Loss : 0.011\n",
      "******************************\n",
      "Source  : paris is mild during summer , but it is usually busy in april .\n",
      "Target  : paris est doux pendant l' été , mais il est généralement occupé en avril .\n",
      "Predict : paris est doux pendant l' été , mais il est généralement occupé en avril .\n",
      "\n",
      "==============================\n",
      "Epoch : 15/200 \n",
      "stop_early : 0 \n",
      "Training Loss : 0.016\n",
      "******************************\n",
      "Source  : she likes mangoes , grapefruit , and pears .\n",
      "Target  : elle aime la mangue , le pamplemousse et les poires .\n",
      "Predict : elle aime la mangue , le pamplemousse et les poires .\n",
      "\n",
      "==============================\n",
      "Epoch : 16/200 \n",
      "stop_early : 0 \n",
      "Training Loss : 0.022\n",
      "******************************\n",
      "Source  : new jersey is sometimes hot during march , and it is beautiful in fall .\n",
      "Target  : new jersey est parfois chaud en mars , et il est beau à l' automne .\n",
      "Predict : new jersey est parfois chaud en mars , et il est beau à l' automne .\n",
      "\n",
      "==============================\n",
      "Epoch : 17/200 \n",
      "stop_early : 0 \n",
      "Training Loss : 0.008\n",
      "******************************\n",
      "Source  : the strawberry is their least favorite fruit , but the apple is our least favorite.\n",
      "Target  : la fraise est leur fruit préféré moins , mais la pomme est notre moins préféré .\n",
      "Predict : la fraise est leur fruit préféré moins , mais la pomme est notre moins préféré .\n",
      "\n",
      "==============================\n",
      "Epoch : 18/200 \n",
      "stop_early : 0 \n",
      "Training Loss : 0.010\n",
      "******************************\n",
      "Source  : india is sometimes cold during march , but it is sometimes hot in january .\n",
      "Target  : l' inde est parfois froid au mois de mars , mais il est parfois chaud en janvier .\n",
      "Predict : l' inde est parfois froid au mois de mars , mais il est parfois chaud en janvier .\n",
      "\n",
      "==============================\n",
      "Epoch : 19/200 \n",
      "stop_early : 0 \n",
      "Training Loss : 0.011\n",
      "******************************\n",
      "Source  : she dislikes lemons , strawberries , and grapes .\n",
      "Target  : elle déteste les citrons , les fraises et les raisins .\n",
      "Predict : elle déteste les citrons , les fraises et les raisins .\n",
      "\n",
      "==============================\n",
      "Epoch : 20/200 \n",
      "stop_early : 0 \n",
      "Training Loss : 0.009\n",
      "******************************\n",
      "Source  : france is never chilly during march , and it is wet in april .\n",
      "Target  : la france est jamais froid en mars , et il est humide en avril .\n",
      "Predict : la france est jamais froid en mars , et il est humide en avril .\n",
      "\n",
      "==============================\n",
      "Epoch : 21/200 \n",
      "stop_early : 0 \n",
      "Training Loss : 0.009\n",
      "******************************\n",
      "Source  : the united states is never beautiful during march , and it is usually relaxing in summer .\n",
      "Target  : les états-unis est jamais belle en mars , et il est relaxant habituellement en été .\n",
      "Predict : les états-unis est jamais belle en mars , et il est relaxant habituellement en été .\n",
      "\n",
      "==============================\n",
      "Epoch : 22/200 \n",
      "stop_early : 0 \n",
      "Training Loss : 0.011\n",
      "******************************\n",
      "Source  : i like peaches , grapes , and lemons .\n",
      "Target  : j'aime les pêches , les raisins et les citrons .\n",
      "Predict : j'aime les pêches , les raisins et les citrons .\n",
      "\n",
      "==============================\n",
      "Epoch : 23/200 \n",
      "stop_early : 0 \n",
      "Training Loss : 0.010\n",
      "******************************\n",
      "Source  : california is never pleasant during winter , and it is sometimes wonderful in december .\n",
      "Target  : california est jamais agréable pendant l' hiver , et il est parfois merveilleux en décembre .\n",
      "Predict : california est jamais agréable pendant l' hiver , et il est parfois merveilleux en décembre .\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Epoch : 24/200 \n",
      "stop_early : 0 \n",
      "Training Loss : 0.011\n",
      "******************************\n",
      "Source  : india is chilly during summer , and it is sometimes beautiful in september .\n",
      "Target  : l' inde est froid pendant l' été , et il est parfois beau en septembre .\n",
      "Predict : l' inde est froid pendant l' été , et il est parfois beau en septembre .\n",
      "\n",
      "==============================\n",
      "Epoch : 25/200 \n",
      "stop_early : 0 \n",
      "Training Loss : 0.016\n",
      "******************************\n",
      "Source  : the united states is never wet during january , but it is usually hot in october .\n",
      "Target  : les états-unis est jamais humide en janvier , mais il est généralement chaud en octobre .\n",
      "Predict : les états-unis est jamais humide en janvier , mais il est généralement chaud en octobre .\n",
      "\n",
      "==============================\n",
      "Epoch : 26/200 \n",
      "stop_early : 0 \n",
      "Training Loss : 0.028\n",
      "******************************\n",
      "Source  : india is never chilly during october , but it is usually relaxing in november .\n",
      "Target  : l' inde est jamais froid en octobre , mais il est relaxant habituellement en novembre .\n",
      "Predict : l' inde est jamais froid en octobre , mais il est relaxant habituellement en novembre .\n",
      "\n",
      "==============================\n",
      "Epoch : 27/200 \n",
      "stop_early : 0 \n",
      "Training Loss : 0.009\n",
      "******************************\n",
      "Source  : paris is never freezing during november , but it is wonderful in october .\n",
      "Target  : paris est jamais le gel en novembre , mais il est merveilleux en octobre .\n",
      "Predict : paris est jamais le gel en novembre , mais il est merveilleux en octobre .\n",
      "\n",
      "==============================\n",
      "Epoch : 28/200 \n",
      "stop_early : 0 \n",
      "Training Loss : 0.018\n",
      "******************************\n",
      "Source  : china is sometimes pleasant during march , but it is usually nice in may .\n",
      "Target  : la chine est parfois agréable au mois de mars , mais il est généralement agréable en mai .\n",
      "Predict : la chine est parfois agréable au mois de mars , mais il est généralement agréable en mai .\n",
      "\n",
      "==============================\n",
      "Epoch : 29/200 \n",
      "stop_early : 0 \n",
      "Training Loss : 0.014\n",
      "******************************\n",
      "Source  : he likes strawberries , oranges , and limes .\n",
      "Target  : il aime les fraises , les oranges et les citrons verts .\n",
      "Predict : il aime les fraises , les oranges et les citrons verts .\n",
      "\n",
      "==============================\n",
      "Epoch : 30/200 \n",
      "stop_early : 0 \n",
      "Training Loss : 0.043\n",
      "******************************\n",
      "Source  : india is never chilly during october , but it is usually relaxing in november .\n",
      "Target  : l' inde est jamais froid en octobre , mais il est relaxant habituellement en novembre .\n",
      "Predict : l' inde est jamais froid en octobre , mais il est relaxant habituellement en novembre .\n",
      "\n",
      "==============================\n",
      "Epoch : 31/200 \n",
      "stop_early : 0 \n",
      "Training Loss : 0.026\n",
      "******************************\n",
      "Source  : india is never chilly during october , but it is usually relaxing in november .\n",
      "Target  : l' inde est jamais froid en octobre , mais il est relaxant habituellement en novembre .\n",
      "Predict : l' inde est jamais froid en octobre , mais il est relaxant habituellement en novembre .\n",
      "\n",
      "==============================\n",
      "Epoch : 32/200 \n",
      "stop_early : 0 \n",
      "Training Loss : 0.023\n",
      "******************************\n",
      "Source  : california is never pleasant during winter , and it is sometimes wonderful in december .\n",
      "Target  : california est jamais agréable pendant l' hiver , et il est parfois merveilleux en décembre .\n",
      "Predict : california est jamais agréable pendant l' hiver , et il est parfois merveilleux en décembre .\n",
      "\n",
      "==============================\n",
      "Epoch : 33/200 \n",
      "stop_early : 0 \n",
      "Training Loss : 0.038\n",
      "******************************\n",
      "Source  : the apple is our least favorite fruit , but the orange is her least favorite .\n",
      "Target  : la pomme est notre fruit préféré moins , mais l'orange est son moins préféré .\n",
      "Predict : la pomme est notre fruit préféré moins , mais l'orange est son moins préféré .\n",
      "\n",
      "==============================\n",
      "Epoch : 34/200 \n",
      "stop_early : 0 \n",
      "Training Loss : 0.042\n",
      "******************************\n",
      "Source  : france is wonderful during november , but it is sometimes hot in september .\n",
      "Target  : france est merveilleux au mois de novembre , mais il est parfois chaud en septembre .\n",
      "Predict : france est merveilleux au mois de novembre , mais il est parfois chaud en septembre .\n",
      "\n",
      "==============================\n",
      "Epoch : 35/200 \n",
      "stop_early : 0 \n",
      "Training Loss : 0.080\n",
      "******************************\n",
      "Source  : the lemon is my most loved fruit , but the strawberry is our most loved .\n",
      "Target  : le citron est mon fruit le plus aimé , mais la fraise est notre plus aimé .\n",
      "Predict : le citron est mon fruit le plus aimé , mais la fraise est notre plus aimé .\n",
      "\n",
      "==============================\n",
      "Epoch : 36/200 \n",
      "stop_early : 0 \n",
      "Training Loss : 0.116\n",
      "******************************\n",
      "Source  : elephants were his most feared animals .\n",
      "Target  : les éléphants étaient ses animaux les plus redoutés .\n",
      "Predict : les éléphants étaient ses animaux les plus redoutés .\n",
      "\n",
      "==============================\n",
      "Epoch : 37/200 \n",
      "stop_early : 0 \n",
      "Training Loss : 0.137\n",
      "******************************\n",
      "Source  : she likes mangoes , apples , and bananas .\n",
      "Target  : elle aime les mangues , les pommes et les bananes .\n",
      "Predict : elle aime les mangues , les pommes et les bananes .\n",
      "\n",
      "==============================\n",
      "Epoch : 38/200 \n",
      "stop_early : 1 \n",
      "Training Loss : 0.151\n",
      "******************************\n",
      "Source  : the strawberry is their least favorite fruit , but the apple is our least favorite.\n",
      "Target  : la fraise est leur fruit préféré moins , mais la pomme est notre moins préféré .\n",
      "Predict : la fraise est leur fruit préféré moins , mais la pomme est notre moins préféré .\n",
      "\n",
      "==============================\n",
      "Epoch : 39/200 \n",
      "stop_early : 2 \n",
      "Training Loss : 0.116\n",
      "******************************\n",
      "Source  : paris is never busy during summer , but it is never freezing in november .\n",
      "Target  : paris est jamais occupé pendant l' été , mais il gèle jamais en novembre .\n",
      "Predict : paris est jamais occupé pendant l' été , mais il gèle jamais en novembre .\n",
      "\n",
      "==============================\n",
      "Epoch : 40/200 \n",
      "stop_early : 3 \n",
      "Training Loss : 0.141\n",
      "******************************\n",
      "Source  : she disliked a rusty yellow car .\n",
      "Target  : elle n'aimait pas une voiture jaune rouillée .\n",
      "Predict : elle n'aimait pas une voiture jaune rouillée .\n",
      "\n",
      "==============================\n",
      "Epoch : 41/200 \n",
      "stop_early : 4 \n",
      "Training Loss : 0.117\n",
      "******************************\n",
      "Source  : china is usually nice during april , and it is never snowy in august .\n",
      "Target  : chine est généralement agréable en avril , et il est jamais de neige en août .\n",
      "Predict : chine est généralement agréable en avril , et il est jamais de neige en août .\n",
      "\n",
      "==============================\n",
      "Epoch : 42/200 \n",
      "stop_early : 5 \n",
      "Training Loss : 0.125\n",
      "******************************\n",
      "Source  : paris is never hot during summer , and it is usually mild in winter .\n",
      "Target  : paris est jamais chaude pendant l' été , et il est généralement doux en hiver .\n",
      "Predict : paris est jamais chaude pendant l' été , et il est généralement doux en hiver .\n",
      "\n",
      "==============================\n",
      "Epoch : 43/200 \n",
      "stop_early : 6 \n",
      "Training Loss : 0.116\n",
      "******************************\n",
      "Source  : india is usually dry during april , and it is freezing in february .\n",
      "Target  : l' inde est généralement sec en avril , et il gèle en février .\n",
      "Predict : l' inde est généralement sec en avril , et il gèle en février .\n",
      "\n",
      "==============================\n",
      "Epoch : 44/200 \n",
      "stop_early : 7 \n",
      "Training Loss : 0.174\n",
      "******************************\n",
      "Source  : she likes peaches , limes , and mangoes .\n",
      "Target  : elle aime les pêches , citrons verts et les mangues .\n",
      "Predict : elle aime les pêches , citrons verts et les mangues .\n",
      "\n",
      "==============================\n",
      "Epoch : 45/200 \n",
      "stop_early : 8 \n",
      "Training Loss : 0.087\n",
      "******************************\n",
      "Source  : she likes lemons , pears , and oranges.\n",
      "Target  : elle aime les citrons , les poires et les oranges .\n",
      "Predict : elle aime les citrons , les poires et les oranges .\n",
      "\n",
      "==============================\n",
      "Epoch : 46/200 \n",
      "stop_early : 9 \n",
      "Training Loss : 0.086\n",
      "******************************\n",
      "Source  : we like peaches , pears , and strawberries .\n",
      "Target  : nous aimons les pêches , les poires et les fraises .\n",
      "Predict : nous aimons les pêches , les poires et les fraises .\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Epoch : 47/200 \n",
      "stop_early : 10 \n",
      "Training Loss : 0.080\n",
      "******************************\n",
      "Source  : india is usually pleasant during november , but it is never relaxing in july .\n",
      "Target  : l' inde est généralement agréable en novembre , mais il est jamais relaxant en juillet .\n",
      "Predict : l' inde est généralement agréable en novembre , mais il est jamais relaxant en juillet .\n",
      "\n",
      "==============================\n",
      "Epoch : 48/200 \n",
      "stop_early : 11 \n",
      "Training Loss : 0.083\n",
      "******************************\n",
      "Source  : she likes strawberries , oranges , and bananas .\n",
      "Target  : elle aime les fraises , les oranges et les bananes .\n",
      "Predict : elle aime les fraises , les oranges et les bananes .\n",
      "\n",
      "==============================\n",
      "Epoch : 49/200 \n",
      "stop_early : 12 \n",
      "Training Loss : 0.104\n",
      "******************************\n",
      "Source  : paris is mild during summer , but it is usually busy in april .\n",
      "Target  : paris est doux pendant l' été , mais il est généralement occupé en avril .\n",
      "Predict : paris est doux pendant l' été , mais il est généralement occupé en avril .\n",
      "\n",
      "==============================\n",
      "Epoch : 50/200 \n",
      "stop_early : 13 \n",
      "Training Loss : 0.069\n",
      "******************************\n",
      "Source  : new jersey is never wet during november , and it is mild in august .\n",
      "Target  : new jersey est jamais humide au mois de novembre , et il est doux au mois d' août .\n",
      "Predict : new jersey est jamais humide au mois de novembre , et il est doux en août d' août .\n",
      "\n",
      "==============================\n",
      "Epoch : 51/200 \n",
      "stop_early : 14 \n",
      "Training Loss : 0.100\n",
      "******************************\n",
      "Source  : paris is usually snowy during winter , and it is sometimes beautiful in august .\n",
      "Target  : paris est généralement enneigée en hiver , et il est parfois beau en août .\n",
      "Predict : paris est généralement enneigée en hiver , et il est parfois beau en août .\n",
      "\n",
      "==============================\n",
      "Epoch : 52/200 \n",
      "stop_early : 15 \n",
      "Training Loss : 0.091\n",
      "******************************\n",
      "Source  : the strawberry is their least favorite fruit , but the apple is our least favorite.\n",
      "Target  : la fraise est leur fruit préféré moins , mais la pomme est notre moins préféré .\n",
      "Predict : la fraise est leur fruit préféré moins , mais la pomme est notre moins préféré .\n",
      "\n",
      "==============================\n",
      "Epoch : 53/200 \n",
      "stop_early : 16 \n",
      "Training Loss : 0.081\n",
      "******************************\n",
      "Source  : france is usually quiet during november , but it is sometimes warm in february .\n",
      "Target  : la france est généralement calme en novembre , mais il est parfois chaud en février .\n",
      "Predict : la france est généralement calme en novembre , mais il est parfois chaud en février .\n",
      "\n",
      "==============================\n",
      "Epoch : 54/200 \n",
      "stop_early : 17 \n",
      "Training Loss : 0.097\n",
      "******************************\n",
      "Source  : we like strawberries , bananas , and oranges .\n",
      "Target  : nous aimons les fraises , les bananes et les oranges .\n",
      "Predict : nous aimons les fraises , les bananes et les oranges .\n",
      "\n",
      "==============================\n",
      "Epoch : 55/200 \n",
      "stop_early : 18 \n",
      "Training Loss : 0.071\n",
      "******************************\n",
      "Source  : the orange is her least liked fruit , but the grapefruit is their least liked .\n",
      "Target  : l'orange est la moins aimé des fruits , mais le pamplemousse est leur moins aimé .\n",
      "Predict : l'orange est la moins aimé des fruits , mais le pamplemousse est leur moins aimé .\n",
      "\n",
      "==============================\n",
      "Epoch : 56/200 \n",
      "stop_early : 19 \n",
      "Training Loss : 0.103\n",
      "******************************\n",
      "Source  : paris is sometimes wonderful during autumn , and it is warm in november .\n",
      "Target  : paris est parfois merveilleux en automne , et il est chaud en novembre .\n",
      "Predict : paris est parfois merveilleux en cours de l' automne , et il est chaud en novembre .\n",
      "\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "prob = 1e-3\n",
    "alpha = 1.2\n",
    "stop_early = 0\n",
    "for epoch_i in range(0 , epochs):\n",
    "    \n",
    "    # 在每進行一個epoch前，把每個batch的index先決定出來\n",
    "    batch_index = []\n",
    "    temp = []\n",
    "    count = 0 \n",
    "    while len(batch_index) <= 1059: \n",
    "        temp.append(count)\n",
    "        count += 1\n",
    "        if len(temp) == batch_size:\n",
    "            batch_index.append(temp)\n",
    "            temp = []\n",
    "        if count == len(train_source):\n",
    "            count = 0\n",
    "            \n",
    "    if alpha * prob < 1  : prob = alpha * prob # prob會隨著epoch增加越來越大\n",
    "    elif alpha * prob > 1: prob = 1.\n",
    "\n",
    "    for batch_i in range(0 , 1059):\n",
    "        train_source_batch_pad , \\\n",
    "        train_target_batch_pad , \\\n",
    "        train_source_batch_length , \\\n",
    "        train_target_batch_length = get_batches(source = train_source ,\n",
    "                                                target = train_target , \n",
    "                                                index = batch_index[batch_i] , \n",
    "                                                on_train = True)\n",
    "        \n",
    "        _ , loss =\\\n",
    "        sess.run([train_op , cost],\n",
    "                 feed_dict = {input_data : train_source_batch_pad ,\n",
    "                              targets : train_target_batch_pad ,\n",
    "                              source_sequence_length : train_source_batch_length ,\n",
    "                              target_sequence_length : train_target_batch_length , \n",
    "                              from_model_or_target : prob})\n",
    "        \n",
    "    valid_source_pad , \\\n",
    "    valid_target_pad , \\\n",
    "    valid_source_length , \\\n",
    "    valid_target_length = get_batches(source = valid_source , target = valid_target)\n",
    "\n",
    "    # 隨便把值丟進targets與target_sequence_length即可\n",
    "    # 因為在測試的時候，這兩項是不必要的\n",
    "    predicting_logits_result =\\\n",
    "    sess.run(predicting_logits ,\n",
    "             feed_dict = {input_data : valid_source_pad ,\n",
    "                          source_sequence_length : valid_source_length ,\n",
    "                          from_model_or_target : 1. ,\n",
    "                          targets : valid_target_pad ,\n",
    "                          target_sequence_length : valid_target_length})\n",
    "\n",
    "    print('=' *  30)\n",
    "    print('Epoch : {}/{} \\nstop_early : {} \\nTraining Loss : {:.3f}'\n",
    "          .format(epoch_i , epochs , stop_early , loss))\n",
    "\n",
    "    index = np.random.randint(batch_size) # 隨機決定一筆data，查看翻譯的結果\n",
    "\n",
    "    source_visualization = []\n",
    "    for i in valid_source_pad[index]:\n",
    "        if source_int_to_letter[i] == '<PAD>': break\n",
    "        source_visualization.append(source_int_to_letter[i])\n",
    "    source_visualization = ' '.join(source_visualization)\n",
    "\n",
    "    target_visualization = []\n",
    "    for i in valid_target_pad[index]:\n",
    "        if target_int_to_letter[i] == '<EOS>': break\n",
    "        target_visualization.append(target_int_to_letter[i])\n",
    "    target_visualization = ' '.join(target_visualization)\n",
    "\n",
    "    predict_visualization = []\n",
    "    for i in predicting_logits_result[index]:\n",
    "        if target_int_to_letter[i] == '<EOS>': break\n",
    "        predict_visualization.append(target_int_to_letter[i])\n",
    "    predict_visualization = ' '.join(predict_visualization)\n",
    "\n",
    "    print('*' *  30)    \n",
    "    print('Source  : {}'.format(source_visualization))\n",
    "    print('Target  : {}'.format(target_visualization))\n",
    "    print('Predict : {}\\n'.format(predict_visualization))\n",
    "    \n",
    "    # 當prob為1的次數為20時即停止計算\n",
    "    if prob == 1: stop_early += 1\n",
    "    if stop_early == 20: break\n",
    "\n",
    "# 保存模型\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess , 'trained_model/save_net')\n",
    "print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from trained_model\\save_net\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "new_saver = tf.train.import_meta_graph(os.path.join('trained_model/save_net.meta'))\n",
    "new_saver.restore(sess, tf.train.latest_checkpoint(os.path.join('trained_model')))\n",
    "\n",
    "graph = tf.get_default_graph()\n",
    "input_data = graph.get_tensor_by_name('inputs:0')\n",
    "targets = graph.get_tensor_by_name('targets:0')\n",
    "source_sequence_length = graph.get_tensor_by_name('source_sequence_length:0')\n",
    "target_sequence_length = graph.get_tensor_by_name('target_sequence_length:0')\n",
    "logits = graph.get_tensor_by_name('decoder_1/predictions:0')\n",
    "from_model_or_target = graph.get_tensor_by_name('from_model_or_target:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "輸入的英文句子 : i dislike grapefruit , lemons , and peaches .\n",
      "google翻譯的法文句子 : je n'aime pamplemousses , les citrons et les pêches .\n",
      "model翻譯的法文句子  : je n'aime pamplemousses , les citrons et les pêches .\n"
     ]
    }
   ],
   "source": [
    "input_sentence = 'i dislike grapefruit , lemons , and peaches .'\n",
    "test_source = []\n",
    "for letter in input_sentence.split(' '):\n",
    "    if letter in source_letter_to_int.keys():\n",
    "        test_source.append(source_letter_to_int[letter])\n",
    "    elif letter not in source_letter_to_int.values():\n",
    "        test_source.append(source_letter_to_int['<UNK>'])\n",
    "test_source = [test_source] * batch_size\n",
    "test_source_length = [len(i) for i in test_source]\n",
    "         \n",
    "# test_target輸入的值可以隨便選，只要長度為target_max_length即可\n",
    "# 雖然這一項對於在inference時不需要，但是還是要把這一項丟進 NN    \n",
    "test_target = [0 for _ in range(0 , 100)] \n",
    "test_target = [test_target] * batch_size\n",
    "test_target_length = [len(i) for i in test_target]\n",
    "\n",
    "test_source = np.array(test_source)\n",
    "test_target = np.array(test_target)\n",
    "test_source_length = np.array(test_source_length)\n",
    "test_target_length = np.array(test_target_length)\n",
    "\n",
    "answer = sess.run(logits , feed_dict = {input_data : test_source ,\n",
    "                                        source_sequence_length : valid_source_length , \n",
    "                                        from_model_or_target : 1. ,          \n",
    "                                        targets : test_target ,\n",
    "                                        target_sequence_length : test_target_length})\n",
    "\n",
    "answer = answer[0 , :]\n",
    "answer_to_letter = []\n",
    "for num in answer:\n",
    "    if target_int_to_letter[num] == '<EOS>': break\n",
    "    answer_to_letter.append(target_int_to_letter[num])\n",
    "\n",
    "print('輸入的英文句子 : {}'.format(input_sentence))  \n",
    "print('google翻譯的法文句子 : {}'.format(\"je n'aime pamplemousses , les citrons et les pêches .\"))  \n",
    "print('model翻譯的法文句子  : {}'.format(' '.join(answer_to_letter)))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "輸入的英文句子 : california is never cold during december , but it is usually warm in may .\n",
      "google翻譯的法文句子 : la californie n'est jamais froide en décembre, mais il fait généralement chaud en mai.\n",
      "model翻譯的法文句子  : californie ne fait jamais froid en décembre , mais il est habituellement chaud en mai .\n"
     ]
    }
   ],
   "source": [
    "input_sentence = 'california is never cold during december , but it is usually warm in may .'\n",
    "test_source = []\n",
    "for letter in input_sentence.split(' '):\n",
    "    if letter in source_letter_to_int.keys():\n",
    "        test_source.append(source_letter_to_int[letter])\n",
    "    elif letter not in source_letter_to_int.values():\n",
    "        test_source.append(source_letter_to_int['<UNK>'])\n",
    "test_source = [test_source] * batch_size\n",
    "test_source_length = [len(i) for i in test_source]\n",
    "         \n",
    "# test_target輸入的值可以隨便選，只要長度為target_max_length即可\n",
    "# 雖然這一項對於在inference時不需要，但是還是要把這一項丟進 NN    \n",
    "test_target = [0 for _ in range(0 , 100)] \n",
    "test_target = [test_target] * batch_size\n",
    "test_target_length = [len(i) for i in test_target]\n",
    "\n",
    "test_source = np.array(test_source)\n",
    "test_target = np.array(test_target)\n",
    "test_source_length = np.array(test_source_length)\n",
    "test_target_length = np.array(test_target_length)\n",
    "\n",
    "answer = sess.run(logits , feed_dict = {input_data : test_source ,\n",
    "                                        source_sequence_length : valid_source_length , \n",
    "                                        from_model_or_target : 1. ,          \n",
    "                                        targets : test_target ,\n",
    "                                        target_sequence_length : test_target_length})\n",
    "\n",
    "answer = answer[0 , :]\n",
    "answer_to_letter = []\n",
    "for num in answer:\n",
    "    if target_int_to_letter[num] == '<EOS>': break\n",
    "    answer_to_letter.append(target_int_to_letter[num])\n",
    "\n",
    "print('輸入的英文句子 : {}'.format(input_sentence))  \n",
    "print('google翻譯的法文句子 : {}'.format(\"la californie n'est jamais froide en décembre, mais il fait généralement chaud en mai.\"))  \n",
    "print('model翻譯的法文句子  : {}'.format(' '.join(answer_to_letter)))      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
