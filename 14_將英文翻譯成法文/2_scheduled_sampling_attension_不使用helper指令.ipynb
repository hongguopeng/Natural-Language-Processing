{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MPNd6FVQ_NNi"
   },
   "source": [
    "# Seq2Seq_Attension\n",
    "實現機器翻譯(Machine Translation)，輸入一句英文句子，輸出一句法文句子\n",
    "<br>輸入 : his least liked fruit is the apple , but your least liked is the strawberry .\n",
    "<br>輸出 : son fruit est moins aimé la pomme , mais votre moins aimé est la fraise .\n",
    "<br>在這支程式中加入Attension機制，強化Seq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wZJjQAmK_NNk"
   },
   "source": [
    "本程式試著不使用Tensorflow所提供的Helper指令，完成Scheduled Sampling<br>\n",
    "Scheduled Sampling是一種解決訓練和生成時輸入數據分布不一致的方法<br>\n",
    "\n",
    "在一般的Seq2Seq模型的inference階段中，如果Sequence中在t時刻中產生錯誤的值，在t時刻之後的輸入狀態將會受到影響，而該誤差會隨著生成過程不斷向後累積；而Scheduled Sampling以一定概率將Decoder自己產生的值作為Decoder端的輸入，這樣即使前面產生錯誤的值，其目標仍然是最大化真實目標序列的概率，模型會朝著正確的方向進<br>\n",
    "\n",
    "在訓練早期Scheduled Sampling主要使用target中的真實值作為Decoder端的輸入，可以將模型從隨機初始化的狀態快速引導至一個合理的狀態；隨著訓練的進行，該方法會逐漸更多地使用Decoder自己產生的值作為Decoder端的輸入，以解決數據分布不一致的問題<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ZV4lBSr_NNn"
   },
   "source": [
    "<img src=\"2_scheduled_sampling_attension_不使用helper指令.jpg\" style=\"width:1140px;height:600px;float:middle\"><br>\n",
    "以上為Decoder端的計算流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2475,
     "status": "ok",
     "timestamp": 1598180873056,
     "user": {
      "displayName": "tomtom7852@kimo.com",
      "photoUrl": "",
      "userId": "13075538237903626622"
     },
     "user_tz": -480
    },
    "id": "i331IzgC_NNp",
    "outputId": "e0756b24-49c1-45e6-f335-a08af94df5d6"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import copy\n",
    "import random\n",
    "from tensorflow.python.layers.core import Dense\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lSmAQbFM_NOb"
   },
   "source": [
    "# 讀取數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vmwtm5jP_NOe"
   },
   "outputs": [],
   "source": [
    "# English source data\n",
    "f_eng = open('data/small_vocab_en', 'r', encoding='utf-8')\n",
    "source_letter = []\n",
    "source_sentence = []\n",
    "while True:\n",
    "    raw = f_eng.readline()\n",
    "    if raw == '' : break\n",
    "\n",
    "    sentence = raw.split('\\n')[0] \n",
    "    temp_sentence = []\n",
    "    for word in sentence.split(' '):\n",
    "        if len(word) != 0:\n",
    "            source_letter.append(word.lower())\n",
    "            temp_sentence.append(word.lower())\n",
    "    source_sentence.append(temp_sentence)      \n",
    "    \n",
    "# French target data\n",
    "f_fre = open('data/small_vocab_fr', 'r', encoding='utf-8')       \n",
    "target_letter = []\n",
    "target_sentence = []\n",
    "while True:\n",
    "    raw = f_fre.readline()\n",
    "    if raw == '' : break\n",
    "\n",
    "    sentence = raw.split('\\n')[0]   \n",
    "    temp_sentence = []\n",
    "    for word in sentence.split(' '):\n",
    "        if len(word) != 0:\n",
    "            target_letter.append(word.lower())\n",
    "            temp_sentence.append(word.lower())\n",
    "    target_sentence.append(temp_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AghKwW8E_NPK"
   },
   "source": [
    "# 數據預處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bs3f7ntC_NPN"
   },
   "outputs": [],
   "source": [
    "special_words = ['<PAD>' , '<UNK>' , '<GO>' , '<EOS>']\n",
    "\n",
    "# 建造英文詞庫\n",
    "source_letter = list(set(source_letter)) + special_words[:2] # 加入 '<PAD>' , '<UNK>'              \n",
    "source_letter_to_int = {word : idx for idx , word in enumerate(source_letter)}   \n",
    "source_int_to_letter = {idx : word for idx , word in enumerate(source_letter)}   \n",
    "\n",
    "# 建造法文詞庫\n",
    "target_letter = list(set(target_letter)) + special_words # 加入 '<PAD>' , '<UNK>' , '<GO>' , '<EOS>'       \n",
    "target_letter_to_int = {word : idx for idx , word in enumerate(target_letter)}   \n",
    "target_int_to_letter = {idx : word for idx , word in enumerate(target_letter)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ilKXPc6_NQC"
   },
   "outputs": [],
   "source": [
    "# 將所有英文詞彙轉換成index\n",
    "source_int = []\n",
    "for sentence in source_sentence:\n",
    "    temp = []\n",
    "    for letter in sentence:\n",
    "        if letter in source_letter_to_int.keys():\n",
    "            temp.append(source_letter_to_int[letter])  \n",
    "        else:\n",
    "            temp.append(source_letter_to_int['<UNK>'])\n",
    "    source_int.append(temp)           \n",
    "\n",
    "# 將所有英文詞彙轉換成index\n",
    "target_int = []\n",
    "for sentence in target_sentence:\n",
    "    temp = []\n",
    "    for letter in sentence:\n",
    "        if letter in target_letter_to_int.keys():\n",
    "            temp.append(target_letter_to_int[letter])\n",
    "        else:\n",
    "            temp.append(target_letter_to_int['<UNK>'])\n",
    "    temp.append(target_letter_to_int['<EOS>'])    \n",
    "    target_int.append(temp)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3U_D38dz_NQc"
   },
   "outputs": [],
   "source": [
    "# 決定source_int與target_int中的最大長度\n",
    "# 因為後面的decoder的rnn不是使用tf.nn.dynamic_rnn，無法使用動態長度的功能，所以在這裡就要決定每個batch的長度\n",
    "source_max_length , target_max_length = 0 , 0  \n",
    "for vob_source , vob_target in zip(source_int , target_int):\n",
    "    if len(vob_source) > source_max_length:\n",
    "        source_max_length = len(vob_source)    \n",
    "    if len(vob_target) > target_max_length:\n",
    "        target_max_length = len(vob_target)  \n",
    "\n",
    "# 分別對source_int與target_int_pad 補source_letter_to_int['<PAD>']與target_letter_to_int['<PAD>']到最大長度  \n",
    "source_int_pad , target_int_pad = [] , []\n",
    "for source_sentence , target_sentence in zip(source_int , target_int):\n",
    "    temp_source = source_sentence.copy()\n",
    "    while len(temp_source) < source_max_length:\n",
    "        temp_source.append(source_letter_to_int['<PAD>']) \n",
    "    source_int_pad.append(temp_source)\n",
    "    \n",
    "    temp_target = target_sentence.copy()\n",
    "    while len(temp_target) < target_max_length:\n",
    "        temp_target.append(target_letter_to_int['<PAD>']) \n",
    "    target_int_pad.append(temp_target)       \n",
    "\n",
    "source_int_pad = np.array(source_int_pad)\n",
    "target_int_pad = np.array(target_int_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o-HOWG1T_NQs"
   },
   "outputs": [],
   "source": [
    "# 超參數\n",
    "# Number of Epochs\n",
    "epochs = 200\n",
    "# Batch Size\n",
    "batch_size = 130\n",
    "# Attention Size\n",
    "att_size = 40\n",
    "# RNN Size\n",
    "rnn_hidden_unit = 128\n",
    "# Number of Layers\n",
    "num_layers = 1\n",
    "# Embedding Size\n",
    "encoding_embedding_size = 100\n",
    "decoding_embedding_size = rnn_hidden_unit\n",
    "source_vocab_size = len(source_int_to_letter)\n",
    "target_vocab_size = len(target_int_to_letter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0AALCeLL_NRU"
   },
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1b3rt25N_NRX"
   },
   "source": [
    "## 輸入層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_KOYl_k9_NRd"
   },
   "outputs": [],
   "source": [
    "input_data = tf.placeholder(tf.int32, [None , source_max_length] , name = 'inputs')\n",
    "targets = tf.placeholder(tf.int32, [None , target_max_length] , name = 'targets')\n",
    "targets_onehot = tf.one_hot(tf.reshape(targets , [-1]) , depth = target_vocab_size , name = 'targets_onehot')\n",
    "target_sequence_length = tf.placeholder(tf.int32 , [None ,] , name = 'target_sequence_length')\n",
    "\n",
    "# 決定到底是\"t-1階段的輸出\"還是\"target中的真實答案\"，當作t階段的輸入\n",
    "from_model_or_target = tf.placeholder(tf.bool , [target_max_length , ] , name = 'from_model_or_target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H9NLUYCR_NR0"
   },
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fCKvxy7u_NSN"
   },
   "source": [
    "需要對source數據進行embedding，再傳入Decoder中的RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8806,
     "status": "ok",
     "timestamp": 1598180879836,
     "user": {
      "displayName": "tomtom7852@kimo.com",
      "photoUrl": "",
      "userId": "13075538237903626622"
     },
     "user_tz": -480
    },
    "id": "WFWqLj6b_NSZ",
    "outputId": "1355a279-d0af-4cfc-a53a-6007ae9bab0f"
   },
   "outputs": [],
   "source": [
    "encoder_embeddings = tf.Variable(tf.random_uniform([source_vocab_size , encoding_embedding_size]))\n",
    "encoder_embed_input = tf.nn.embedding_lookup(encoder_embeddings , input_data)\n",
    "\n",
    "def get_lstm_cell(rnn_hidden_unit):\n",
    "    lstm_cell = tf.contrib.rnn.LSTMCell(rnn_hidden_unit, \n",
    "                                        initializer = tf.random_uniform_initializer(-0.1 , 0.1))\n",
    "    return lstm_cell\n",
    "\n",
    "with tf.variable_scope('encoder'):   \n",
    "    encoder_cell = tf.contrib.rnn.MultiRNNCell([get_lstm_cell(rnn_hidden_unit) for _ in range(num_layers)])\n",
    "    \n",
    "    encoder_output, encoder_state = tf.nn.dynamic_rnn(encoder_cell , \n",
    "                                                      encoder_embed_input , \n",
    "                                                      dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XIBw6Ib8_NTW"
   },
   "source": [
    "## Decoder and Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9OSDugYz_NTY"
   },
   "outputs": [],
   "source": [
    "# 預處理後的decoder輸入\n",
    "# 在batch中每一筆data最前面加上<GO>，並移除最後一個字，所以每一筆data的詞的數目並無改變\n",
    "ending = tf.identity(targets[: , 0:-1])\n",
    "decoder_input = tf.concat([tf.fill([batch_size, 1] , target_letter_to_int['<GO>']) , ending] , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JaGgmQ-u_NTm"
   },
   "outputs": [],
   "source": [
    "# 1. Embedding\n",
    "decoder_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n",
    "decoder_embed_input = tf.nn.embedding_lookup(decoder_embeddings , decoder_input)\n",
    "\n",
    "with tf.variable_scope('decoder'):\n",
    "    # 2. 建構Decoder中的RNN\n",
    "    decoder_cell = tf.contrib.rnn.MultiRNNCell([get_lstm_cell(rnn_hidden_unit) for _ in range(num_layers)])\n",
    "    state = encoder_state \n",
    "    outputs  = []\n",
    "    final_outputs  = []\n",
    "    attention_matrix = []\n",
    "    for time_step in range(0 , target_max_length):\n",
    "        if time_step > 0: tf.get_variable_scope().reuse_variables()  \n",
    "        weights = tf.get_variable(initializer = tf.truncated_normal([rnn_hidden_unit , target_vocab_size] , mean = 0.01 , stddev = 0.1) , name = 'weights_decoder')\n",
    "        biases = tf.get_variable(initializer = tf.zeros([1 , target_vocab_size]) + 0.0001 , name = 'biases_decoder')\n",
    "            \n",
    "        # 在訓練的過程中，除了time_step為0以外，每一個time_step都投擲硬幣決定\n",
    "        # 要用decoder_embed_input[: , time_step , :](target中的真實答案)，還是mlstm_cell_output(machine自己產生的output)，輸入decoder_cell\n",
    "        # 正面代表當前時刻decoder_cell要吃decoder_embed_input[: , time_step , :](target中的真實答案)\n",
    "        # 反面代表當前時刻decoder_cell要吃mlstm_cell_output(上一個時刻decoder_cell的輸出)\n",
    "        # 也就是coin tossing為[True , True , True , True , True , True , True , True]會逐漸變為[Faslse , Faslse , Faslse , Faslse , Faslse , Faslse , Faslse , Faslse]\n",
    "        # 前幾個epoch會有很大的機率擲出正面，但隨著訓練的過程擲出反面的機率會越來越大\n",
    "        # 在後面的步驟會調節擲出正反面的機率\n",
    "        if time_step == 0: \n",
    "            input_to_decoder = decoder_embed_input[: , 0 , :]\n",
    "            mlstm_cell_output , state = decoder_cell(input_to_decoder , state)\n",
    "            output = tf.matmul(mlstm_cell_output , weights) + biases\n",
    "                \n",
    "        elif time_step > 0: \n",
    "            def from_target():\n",
    "                input_to_decoder = decoder_embed_input[: , time_step , :]\n",
    "                h_output , h_state = decoder_cell(input_to_decoder , state)\n",
    "                return h_output , h_state\n",
    "\n",
    "            def form_model():\n",
    "                input_to_decoder = tf.argmax(output , axis = 1)\n",
    "                input_to_decoder = tf.nn.embedding_lookup(decoder_embeddings , input_to_decoder)\n",
    "                h_output , h_state = decoder_cell(input_to_decoder , state)\n",
    "                return h_output , h_state\n",
    "\n",
    "            # mlstm_cell_output → [batch_size , rnn_hidden_unit]\n",
    "            mlstm_cell_output , state = tf.cond(from_model_or_target[time_step] ,\n",
    "                                                from_target ,\n",
    "                                                form_model)  \n",
    "        \n",
    "        with tf.variable_scope('attention'):\n",
    "            if time_step > 0: tf.get_variable_scope().reuse_variables()\n",
    "            W_1 = tf.get_variable(initializer = tf.truncated_normal([rnn_hidden_unit , att_size] , mean = 0.001 , stddev = 0.3) ,\n",
    "                                  name = 'w_1') \n",
    "            W_2 = tf.get_variable(initializer = tf.truncated_normal([rnn_hidden_unit , att_size] , mean = 0.001 , stddev = 0.3) ,\n",
    "                                  name = 'w_2')       \n",
    "            V = tf.get_variable(initializer = tf.truncated_normal([att_size , 1] , mean = 0.001 , stddev = 0.3) ,\n",
    "                                name = 'V')\n",
    "            \n",
    "            # score → [batch_size , source_max_length , att_size]\n",
    "            score = tf.nn.tanh(tf.tensordot(encoder_output , W_1 , axes = 1) + \\\n",
    "                               tf.expand_dims(tf.tensordot(mlstm_cell_output , W_2 , axes = 1) , 1))\n",
    "            \n",
    "            # score → [batch_size , source_max_length , 1]\n",
    "            score = tf.tensordot(score , V , axes = 1) \n",
    "            \n",
    "            # attention_weights → [batch_size , source_max_length , 1]\n",
    "            attention_weights = tf.nn.softmax(score , axis = 1)       \n",
    "            \n",
    "            # context_vector → [batch_size , source_max_length , rnn_hidden_unit]\n",
    "            context_vector = tf.multiply(attention_weights , encoder_output)       \n",
    "            \n",
    "            # context_vector → [batch_size , rnn_hidden_unit]\n",
    "            context_vector = tf.reduce_sum(context_vector , axis = 1)             \n",
    "        \n",
    "        # output → [batch_size , target_vocab_size]\n",
    "        output = tf.matmul(context_vector , weights) + biases\n",
    "        final_outputs.append(output)    \n",
    "\n",
    "        # tf.squeeze(attention_weights , [-1]) : [batch_size , source_max_length , 1] → [batch_size , source_max_length] \n",
    "        attention_matrix.append(tf.squeeze(attention_weights , [-1])) # 把每一個time_step的attention_weights收集起來成為attention_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lz45dh5M_NTv",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_outputs_ = tf.transpose(tf.convert_to_tensor(final_outputs) , [1 , 0 , 2]) # final_outputs_ → [batch_size , target_max_length , rnn_hidden_unit]    \n",
    "logits = tf.reshape(final_outputs_ , [-1 , target_vocab_size]) \n",
    "\n",
    "predicting_logits = tf.nn.softmax(logits , axis = 1)   \n",
    "predicting_logits = tf.argmax(predicting_logits , axis = 1)\n",
    "predicting_logits = tf.reshape(predicting_logits , [batch_size , -1] , name = 'predictions')\n",
    "attention_matrix = tf.transpose(tf.convert_to_tensor(attention_matrix) , [1 , 2 , 0] , name = 'attention_weight') # attention_matrix => [batch_size , source_max_length , target_max_length]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15417,
     "status": "ok",
     "timestamp": 1598180886650,
     "user": {
      "displayName": "tomtom7852@kimo.com",
      "photoUrl": "",
      "userId": "13075538237903626622"
     },
     "user_tz": -480
    },
    "id": "9IvBxQje_NT2",
    "outputId": "eeb9ca84-0f7c-4871-d6aa-f3a7cff78933"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-e45eb798c396>:18: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('optimization'):\n",
    "    '''\n",
    "    target_sequence_length : [4 , 2 , 3]\n",
    "\n",
    "    max_target_sequence_length : 8\n",
    "\n",
    "    ⟹ masks的輸出長這樣 : 1 1 1 1 0 0 0 0  (4)\n",
    "                          1 1 0 0 0 0 0 0  (2)\n",
    "                          1 1 1 0 0 0 0 0  (3)\n",
    "    ➝ 0的部分代表是補0的地方，不列入loss的計算\n",
    "    '''   \n",
    "    masks = tf.sequence_mask(target_sequence_length ,\n",
    "                             target_max_length ,\n",
    "                             dtype = tf.float32,\n",
    "                             name = 'masks')\n",
    "    \n",
    "    # Loss function\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels = targets_onehot , logits = logits)\n",
    "    loss = tf.reshape(loss , [-1 , target_max_length])\n",
    "    loss = tf.multiply(loss , masks)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(loss)\n",
    "    capped_gradients = [(tf.clip_by_value(grad , -1. , 1.) , var) for grad , var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-CGb3y1h_NUZ"
   },
   "outputs": [],
   "source": [
    "# 將數據集分割為train和validation\n",
    "train_source = source_int_pad[batch_size:]\n",
    "train_target = target_int_pad[batch_size:]\n",
    "# 留出一個batch進行驗證\n",
    "valid_source = source_int_pad[:batch_size]\n",
    "valid_target = target_int_pad[:batch_size]\n",
    "valid_target_length =\\\n",
    "np.array(list(map(lambda x: len([y for y in x if y != target_letter_to_int['<PAD>']]) , valid_target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J0T1GFm0_NUt"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1990137,
     "status": "ok",
     "timestamp": 1598182861453,
     "user": {
      "displayName": "tomtom7852@kimo.com",
      "photoUrl": "",
      "userId": "13075538237903626622"
     },
     "user_tz": -480
    },
    "id": "7SJA2UNx_NU3",
    "outputId": "93b80712-c7f3-4b58-be19-338467dd4e65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Epoch : 0/200 \n",
      "Training Loss : 0.550 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 24\n",
      "******************************\n",
      "Source  : the strawberry is their least favorite fruit , but the apple is our least favorite.\n",
      "Target  : la fraise est leur fruit préféré moins , mais la pomme est notre moins préféré .\n",
      "Predict : la fraise est leur fruit préféré moins , mais la fraise est leur moins préféré .\n",
      "\n",
      "==============================\n",
      "Epoch : 1/200 \n",
      "Training Loss : 0.346 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 24\n",
      "******************************\n",
      "Source  : new jersey is usually beautiful during april , and it is never relaxing in december .\n",
      "Target  : new jersey est généralement beau en avril , et il est jamais relaxant en décembre .\n",
      "Predict : new jersey est généralement beau en juin , et il est généralement agréable en décembre .\n",
      "\n",
      "==============================\n",
      "Epoch : 2/200 \n",
      "Training Loss : 0.269 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 24\n",
      "******************************\n",
      "Source  : new jersey is freezing during winter , but it is sometimes wonderful in january .\n",
      "Target  : new jersey est le gel pendant l' hiver , mais il est parfois merveilleux en janvier .\n",
      "Predict : new jersey est le gel pendant l' automne , mais il est parfois le gel en juin .\n",
      "\n",
      "==============================\n",
      "Epoch : 3/200 \n",
      "Training Loss : 0.208 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 24\n",
      "******************************\n",
      "Source  : france is never cold during september , and it is snowy in october .\n",
      "Target  : france ne fait jamais froid en septembre , et il est neigeux en octobre .\n",
      "Predict : la france ne gèle en mars , et il est beau en août .\n",
      "\n",
      "==============================\n",
      "Epoch : 4/200 \n",
      "Training Loss : 0.172 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 24\n",
      "******************************\n",
      "Source  : the lemon is my most loved fruit , but the strawberry is our most loved .\n",
      "Target  : le citron est mon fruit le plus aimé , mais la fraise est notre plus aimé .\n",
      "Predict : le citron est mon fruit le plus aimé , mais la pêche est notre plus aimé .\n",
      "\n",
      "==============================\n",
      "Epoch : 5/200 \n",
      "Training Loss : 0.134 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 24\n",
      "******************************\n",
      "Source  : california is never cold during december , but it is usually warm in may .\n",
      "Target  : californie ne fait jamais froid en décembre , mais il est habituellement chaud en mai .\n",
      "Predict : californie ne fait jamais froid en décembre , mais il est habituellement chaud en mai .\n",
      "\n",
      "==============================\n",
      "Epoch : 6/200 \n",
      "Training Loss : 0.106 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 24\n",
      "******************************\n",
      "Source  : india is sometimes cold during march , but it is sometimes hot in january .\n",
      "Target  : l' inde est parfois froid au mois de mars , mais il est parfois chaud en janvier .\n",
      "Predict : l' inde est parfois froid en mars , mais il est parfois chaud en janvier .\n",
      "\n",
      "==============================\n",
      "Epoch : 7/200 \n",
      "Training Loss : 0.079 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 24\n",
      "******************************\n",
      "Source  : the grapefruit is her least favorite fruit , but the banana is their least favorite .\n",
      "Target  : le pamplemousse est son fruit préféré moins , mais la banane est leur moins préférée .\n",
      "Predict : le pamplemousse est son fruit préféré moins , mais la pomme est leur moins préférée .\n",
      "\n",
      "==============================\n",
      "Epoch : 8/200 \n",
      "Training Loss : 0.070 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 24\n",
      "******************************\n",
      "Source  : the grapefruit is her least favorite fruit , but the banana is their least favorite .\n",
      "Target  : le pamplemousse est son fruit préféré moins , mais la banane est leur moins préférée .\n",
      "Predict : le pamplemousse est son fruit préféré moins , mais la pomme est leur moins préférée .\n",
      "\n",
      "==============================\n",
      "Epoch : 9/200 \n",
      "Training Loss : 0.052 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 23\n",
      "******************************\n",
      "Source  : the grapefruit is our least favorite fruit , but the orange is your least favorite .\n",
      "Target  : le pamplemousse est notre fruit préféré moins , mais l'orange est votre préféré moins .\n",
      "Predict : le pamplemousse est notre fruit préféré moins , mais l'orange est votre préféré moins .\n",
      "\n",
      "==============================\n",
      "Epoch : 10/200 \n",
      "Training Loss : 0.044 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 24\n",
      "******************************\n",
      "Source  : new jersey is never wet during november , and it is mild in august .\n",
      "Target  : new jersey est jamais humide au mois de novembre , et il est doux au mois d' août .\n",
      "Predict : new jersey est jamais humide au mois de novembre , et il est doux au mois d' août .\n",
      "\n",
      "==============================\n",
      "Epoch : 11/200 \n",
      "Training Loss : 0.038 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 23\n",
      "******************************\n",
      "Source  : she likes mangoes , grapefruit , and pears .\n",
      "Target  : elle aime la mangue , le pamplemousse et les poires .\n",
      "Predict : elle aime la mangue , le pamplemousse et les poires .\n",
      "\n",
      "==============================\n",
      "Epoch : 12/200 \n",
      "Training Loss : 0.037 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 24\n",
      "******************************\n",
      "Source  : the united states is sometimes busy during january , and it is sometimes warm in november .\n",
      "Target  : les états-unis est parfois occupé en janvier , et il est parfois chaud en novembre .\n",
      "Predict : les états-unis est parfois occupé en janvier , et il est parfois chaud en novembre .\n",
      "\n",
      "==============================\n",
      "Epoch : 13/200 \n",
      "Training Loss : 0.032 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 24\n",
      "******************************\n",
      "Source  : china is usually dry during march , but it is nice in november .\n",
      "Target  : chine est généralement sec en mars , mais il est agréable en novembre .\n",
      "Predict : chine est généralement sec au mois de mars , mais il est agréable en novembre .\n",
      "\n",
      "==============================\n",
      "Epoch : 14/200 \n",
      "Training Loss : 0.032 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 24\n",
      "******************************\n",
      "Source  : china is warm during spring , and it is sometimes cold in february .\n",
      "Target  : chine est chaud au printemps , et il est parfois froid en février .\n",
      "Predict : chine est chaud au printemps , et il est parfois froid en février .\n",
      "\n",
      "==============================\n",
      "Epoch : 15/200 \n",
      "Training Loss : 0.032 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 24\n",
      "******************************\n",
      "Source  : he dislikes apples , peaches , and grapes .\n",
      "Target  : il déteste les pommes , les pêches et les raisins .\n",
      "Predict : il déteste les pommes , les pêches et les raisins .\n",
      "\n",
      "==============================\n",
      "Epoch : 16/200 \n",
      "Training Loss : 0.030 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 24\n",
      "******************************\n",
      "Source  : india is never chilly during october , but it is usually relaxing in november .\n",
      "Target  : l' inde est jamais froid en octobre , mais il est relaxant habituellement en novembre .\n",
      "Predict : l' inde est jamais froid en octobre , mais il est relaxant habituellement en novembre .\n",
      "\n",
      "==============================\n",
      "Epoch : 17/200 \n",
      "Training Loss : 0.026 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 23\n",
      "******************************\n",
      "Source  : she disliked a rusty yellow car .\n",
      "Target  : elle n'aimait pas une voiture jaune rouillée .\n",
      "Predict : elle n'aimait pas une voiture jaune rouillée .\n",
      "\n",
      "==============================\n",
      "Epoch : 18/200 \n",
      "Training Loss : 0.025 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 23\n",
      "******************************\n",
      "Source  : they like lemons , limes , and grapefruit .\n",
      "Target  : ils aiment les citrons , citrons verts et le pamplemousse .\n",
      "Predict : ils aiment les citrons , citrons verts et le pamplemousse .\n",
      "\n",
      "==============================\n",
      "Epoch : 19/200 \n",
      "Training Loss : 0.028 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 20\n",
      "******************************\n",
      "Source  : she disliked a rusty yellow car .\n",
      "Target  : elle n'aimait pas une voiture jaune rouillée .\n",
      "Predict : elle n'aimait pas une voiture jaune rouillée .\n",
      "\n",
      "==============================\n",
      "Epoch : 20/200 \n",
      "Training Loss : 0.022 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 24\n",
      "******************************\n",
      "Source  : china is freezing during july , but it is relaxing in january .\n",
      "Target  : chine gèle en juillet , mais il est relaxant en janvier .\n",
      "Predict : chine gèle en juillet , mais il est humide en janvier .\n",
      "\n",
      "==============================\n",
      "Epoch : 21/200 \n",
      "Training Loss : 0.022 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 22\n",
      "******************************\n",
      "Source  : your most feared animal is that shark .\n",
      "Target  : votre animal le plus redouté est que le requin .\n",
      "Predict : votre animal le plus redouté est que le requin .\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Epoch : 22/200 \n",
      "Training Loss : 0.020 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 23\n",
      "******************************\n",
      "Source  : california is beautiful during january , and it is pleasant in february .\n",
      "Target  : californie est beau en janvier , et il est agréable en février .\n",
      "Predict : californie est beau en janvier , et il est agréable en février .\n",
      "\n",
      "==============================\n",
      "Epoch : 23/200 \n",
      "Training Loss : 0.032 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 22\n",
      "******************************\n",
      "Source  : the orange is her least liked fruit , but the grapefruit is their least liked .\n",
      "Target  : l'orange est la moins aimé des fruits , mais le pamplemousse est leur moins aimé .\n",
      "Predict : l'orange est la moins aimé des fruits , mais le pamplemousse est leur moins aimé .\n",
      "\n",
      "==============================\n",
      "Epoch : 24/200 \n",
      "Training Loss : 0.016 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 22\n",
      "******************************\n",
      "Source  : india is sometimes cold during march , but it is sometimes hot in january .\n",
      "Target  : l' inde est parfois froid au mois de mars , mais il est parfois chaud en janvier .\n",
      "Predict : l' inde est parfois froid en mars , mais il est parfois chaud en janvier .\n",
      "\n",
      "==============================\n",
      "Epoch : 25/200 \n",
      "Training Loss : 0.024 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 21\n",
      "******************************\n",
      "Source  : your least liked fruit is the grape , but my least liked is the apple .\n",
      "Target  : votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .\n",
      "Predict : votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .\n",
      "\n",
      "==============================\n",
      "Epoch : 26/200 \n",
      "Training Loss : 0.019 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 23\n",
      "******************************\n",
      "Source  : china is never nice during july , but it is usually snowy in spring .\n",
      "Target  : chine est jamais agréable en juillet , mais il est généralement enneigée au printemps .\n",
      "Predict : chine est jamais agréable en juillet , mais il est généralement enneigée au printemps .\n",
      "\n",
      "==============================\n",
      "Epoch : 27/200 \n",
      "Training Loss : 0.021 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 20\n",
      "******************************\n",
      "Source  : the united states is never nice during february , but it is sometimes pleasant in april .\n",
      "Target  : les états-unis est jamais agréable en février , mais il est parfois agréable en avril .\n",
      "Predict : les états-unis est jamais agréable en février , mais il est parfois agréable en avril .\n",
      "\n",
      "==============================\n",
      "Epoch : 28/200 \n",
      "Training Loss : 0.021 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 21\n",
      "******************************\n",
      "Source  : california is busy during november , but it is rainy in autumn .\n",
      "Target  : californie est occupé au mois de novembre , mais il est pluvieux à l' automne .\n",
      "Predict : californie est occupé au mois de novembre , mais il est pluvieux à l' automne .\n",
      "\n",
      "==============================\n",
      "Epoch : 29/200 \n",
      "Training Loss : 0.028 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 18\n",
      "******************************\n",
      "Source  : she dislikes that little red truck .\n",
      "Target  : elle déteste ce petit camion rouge .\n",
      "Predict : elle déteste ce petit camion rouge .\n",
      "\n",
      "==============================\n",
      "Epoch : 30/200 \n",
      "Training Loss : 0.016 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 21\n",
      "******************************\n",
      "Source  : california is busy during november , but it is rainy in autumn .\n",
      "Target  : californie est occupé au mois de novembre , mais il est pluvieux à l' automne .\n",
      "Predict : californie est occupé en novembre , mais il pleut à l' automne .\n",
      "\n",
      "==============================\n",
      "Epoch : 31/200 \n",
      "Training Loss : 0.016 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 18\n",
      "******************************\n",
      "Source  : we like peaches , mangoes , and oranges .\n",
      "Target  : nous aimons les pêches , les mangues et les oranges .\n",
      "Predict : nous aimons les pêches , les mangues et les oranges .\n",
      "\n",
      "==============================\n",
      "Epoch : 32/200 \n",
      "Training Loss : 0.047 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 13\n",
      "******************************\n",
      "Source  : he dislikes grapefruit , mangoes , and lemons .\n",
      "Target  : il n'aime pamplemousse , mangues et citrons .\n",
      "Predict : il aime pas le pamplemousse , les mangues et citrons .\n",
      "\n",
      "==============================\n",
      "Epoch : 33/200 \n",
      "Training Loss : 0.018 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 19\n",
      "******************************\n",
      "Source  : new jersey is freezing during winter , but it is sometimes wonderful in january .\n",
      "Target  : new jersey est le gel pendant l' hiver , mais il est parfois merveilleux en janvier .\n",
      "Predict : new jersey est le gel pendant l' hiver , mais il est parfois merveilleux en janvier .\n",
      "\n",
      "==============================\n",
      "Epoch : 34/200 \n",
      "Training Loss : 0.031 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 11\n",
      "******************************\n",
      "Source  : california is usually freezing during december , and it is busy in april .\n",
      "Target  : la californie est le gel habituellement en décembre , et il est occupé en avril .\n",
      "Predict : la californie est le gel habituellement en décembre , et il est occupé en avril .\n",
      "\n",
      "==============================\n",
      "Epoch : 35/200 \n",
      "Training Loss : 0.059 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 8\n",
      "******************************\n",
      "Source  : new jersey is usually beautiful during april , and it is never relaxing in december .\n",
      "Target  : new jersey est généralement beau en avril , et il est jamais relaxant en décembre .\n",
      "Predict : new jersey est généralement beau en avril , et il est jamais relaxant en décembre .\n",
      "\n",
      "==============================\n",
      "Epoch : 36/200 \n",
      "Training Loss : 0.062 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 5\n",
      "******************************\n",
      "Source  : he dislikes grapefruit , mangoes , and lemons .\n",
      "Target  : il n'aime pamplemousse , mangues et citrons .\n",
      "Predict : il aime pas le pamplemousse , les mangues et citrons .\n",
      "\n",
      "==============================\n",
      "Epoch : 37/200 \n",
      "Training Loss : 0.046 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 3\n",
      "******************************\n",
      "Source  : she disliked a rusty yellow car .\n",
      "Target  : elle n'aimait pas une voiture jaune rouillée .\n",
      "Predict : elle n'aimait pas une voiture jaune rouillée .\n",
      "\n",
      "==============================\n",
      "Epoch : 38/200 \n",
      "Training Loss : 0.108 \n",
      "stop_early : 0 \n",
      "coin_tossing_True_num : 0\n",
      "******************************\n",
      "Source  : we like peaches , mangoes , and oranges .\n",
      "Target  : nous aimons les pêches , les mangues et les oranges .\n",
      "Predict : nous aimons les pêches , les mangues et les oranges .\n",
      "\n",
      "==============================\n",
      "Epoch : 39/200 \n",
      "Training Loss : 0.134 \n",
      "stop_early : 1 \n",
      "coin_tossing_True_num : 0\n",
      "******************************\n",
      "Source  : the orange is her least liked fruit , but the grapefruit is their least liked .\n",
      "Target  : l'orange est la moins aimé des fruits , mais le pamplemousse est leur moins aimé .\n",
      "Predict : l'orange est la moins aimé des fruits , mais le pamplemousse est leur moins aimé .\n",
      "\n",
      "==============================\n",
      "Epoch : 40/200 \n",
      "Training Loss : 0.101 \n",
      "stop_early : 2 \n",
      "coin_tossing_True_num : 0\n",
      "******************************\n",
      "Source  : china is never nice during july , but it is usually snowy in spring .\n",
      "Target  : chine est jamais agréable en juillet , mais il est généralement enneigée au printemps .\n",
      "Predict : chine est jamais agréable en juillet , mais il est généralement enneigée au printemps .\n",
      "\n",
      "==============================\n",
      "Epoch : 41/200 \n",
      "Training Loss : 0.117 \n",
      "stop_early : 3 \n",
      "coin_tossing_True_num : 0\n",
      "******************************\n",
      "Source  : china is warm during spring , and it is sometimes cold in february .\n",
      "Target  : chine est chaud au printemps , et il est parfois froid en février .\n",
      "Predict : chine est chaud au printemps , et il est parfois froid en février .\n",
      "\n",
      "==============================\n",
      "Epoch : 42/200 \n",
      "Training Loss : 0.105 \n",
      "stop_early : 4 \n",
      "coin_tossing_True_num : 0\n",
      "******************************\n",
      "Source  : california is never rainy during winter , and it is usually mild in summer .\n",
      "Target  : california est jamais pluvieux pendant l' hiver , et il est généralement doux en été .\n",
      "Predict : california est jamais pluvieux pendant l' hiver , et il est généralement doux en été .\n",
      "\n",
      "==============================\n",
      "Epoch : 43/200 \n",
      "Training Loss : 0.093 \n",
      "stop_early : 5 \n",
      "coin_tossing_True_num : 0\n",
      "******************************\n",
      "Source  : the grapefruit is my most loved fruit , but the banana is her most loved .\n",
      "Target  : le pamplemousse est mon fruit le plus cher , mais la banane est la plus aimée .\n",
      "Predict : le pamplemousse est mon fruit le plus cher , mais la banane est la plus aimée .\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Epoch : 44/200 \n",
      "Training Loss : 0.091 \n",
      "stop_early : 6 \n",
      "coin_tossing_True_num : 0\n",
      "******************************\n",
      "Source  : the united states is sometimes busy during january , and it is sometimes warm in november .\n",
      "Target  : les états-unis est parfois occupé en janvier , et il est parfois chaud en novembre .\n",
      "Predict : les états-unis est parfois occupé en janvier , et il est parfois chaud en novembre .\n",
      "\n",
      "==============================\n",
      "Epoch : 45/200 \n",
      "Training Loss : 0.107 \n",
      "stop_early : 7 \n",
      "coin_tossing_True_num : 0\n",
      "******************************\n",
      "Source  : new jersey is freezing during winter , but it is sometimes wonderful in january .\n",
      "Target  : new jersey est le gel pendant l' hiver , mais il est parfois merveilleux en janvier .\n",
      "Predict : new jersey est le gel pendant l' hiver , mais il est parfois merveilleux en janvier .\n",
      "\n",
      "==============================\n",
      "Epoch : 46/200 \n",
      "Training Loss : 0.105 \n",
      "stop_early : 8 \n",
      "coin_tossing_True_num : 0\n",
      "******************************\n",
      "Source  : paris is usually beautiful during september , and it is usually snowy in november .\n",
      "Target  : paris est généralement beau en septembre , et il est généralement enneigée en novembre .\n",
      "Predict : paris est généralement beau en septembre , et il est généralement enneigée en novembre .\n",
      "\n",
      "==============================\n",
      "Epoch : 47/200 \n",
      "Training Loss : 0.085 \n",
      "stop_early : 9 \n",
      "coin_tossing_True_num : 0\n",
      "******************************\n",
      "Source  : india is never chilly during october , but it is usually relaxing in november .\n",
      "Target  : l' inde est jamais froid en octobre , mais il est relaxant habituellement en novembre .\n",
      "Predict : l' inde est jamais froid en octobre , mais il est relaxant habituellement en novembre .\n",
      "\n",
      "==============================\n",
      "Epoch : 48/200 \n",
      "Training Loss : 0.091 \n",
      "stop_early : 10 \n",
      "coin_tossing_True_num : 0\n",
      "******************************\n",
      "Source  : california is never wet during november , and it is sometimes pleasant in september .\n",
      "Target  : california est jamais humide en novembre , et il est parfois agréable en septembre .\n",
      "Predict : california est jamais humide en novembre , et il est parfois agréable en septembre .\n",
      "\n",
      "==============================\n",
      "Epoch : 49/200 \n",
      "Training Loss : 0.088 \n",
      "stop_early : 11 \n",
      "coin_tossing_True_num : 0\n",
      "******************************\n",
      "Source  : new jersey is usually quiet during fall , but it is usually warm in april .\n",
      "Target  : new jersey est généralement calme au cours de l' automne , mais il est généralement chaud en avril .\n",
      "Predict : new jersey est généralement calme au cours de l' automne , mais il est habituellement chaud en avril .\n",
      "\n",
      "==============================\n",
      "Epoch : 50/200 \n",
      "Training Loss : 0.078 \n",
      "stop_early : 12 \n",
      "coin_tossing_True_num : 0\n",
      "******************************\n",
      "Source  : paris is never relaxing during march , but it is usually freezing in autumn .\n",
      "Target  : paris est jamais relaxant au mois de mars , mais il gèle habituellement à l' automne .\n",
      "Predict : paris est jamais relaxant au mois de mars , mais il gèle habituellement à l' automne .\n",
      "\n",
      "==============================\n",
      "Epoch : 51/200 \n",
      "Training Loss : 0.065 \n",
      "stop_early : 13 \n",
      "coin_tossing_True_num : 0\n",
      "******************************\n",
      "Source  : the united states is sometimes mild during june , and it is cold in september .\n",
      "Target  : les états-unis est parfois légère en juin , et il fait froid en septembre .\n",
      "Predict : les états-unis est parfois doux en juin , et il fait froid en septembre .\n",
      "\n",
      "==============================\n",
      "Epoch : 52/200 \n",
      "Training Loss : 0.058 \n",
      "stop_early : 14 \n",
      "coin_tossing_True_num : 0\n",
      "******************************\n",
      "Source  : we like oranges , mangoes , and grapes .\n",
      "Target  : nous aimons les oranges , les mangues et les raisins .\n",
      "Predict : nous aimons les oranges , les mangues et les raisins .\n",
      "\n",
      "==============================\n",
      "Epoch : 53/200 \n",
      "Training Loss : 0.088 \n",
      "stop_early : 15 \n",
      "coin_tossing_True_num : 0\n",
      "******************************\n",
      "Source  : paris is mild during summer , but it is usually busy in april .\n",
      "Target  : paris est doux pendant l' été , mais il est généralement occupé en avril .\n",
      "Predict : paris est doux pendant l' été , mais il est généralement occupé en avril .\n",
      "\n",
      "==============================\n",
      "Epoch : 54/200 \n",
      "Training Loss : 0.075 \n",
      "stop_early : 16 \n",
      "coin_tossing_True_num : 0\n",
      "******************************\n",
      "Source  : california is beautiful during january , and it is pleasant in february .\n",
      "Target  : californie est beau en janvier , et il est agréable en février .\n",
      "Predict : californie est beau en janvier , et il est agréable en février .\n",
      "\n",
      "==============================\n",
      "Epoch : 55/200 \n",
      "Training Loss : 0.049 \n",
      "stop_early : 17 \n",
      "coin_tossing_True_num : 0\n",
      "******************************\n",
      "Source  : the united states is sometimes beautiful during november , and it is never rainy in march .\n",
      "Target  : les états-unis est parfois belle au mois de novembre , et il est jamais pluvieux en mars .\n",
      "Predict : les états-unis est parfois belle au mois de novembre , et il est jamais pluvieux en mars .\n",
      "\n",
      "==============================\n",
      "Epoch : 56/200 \n",
      "Training Loss : 0.072 \n",
      "stop_early : 18 \n",
      "coin_tossing_True_num : 0\n",
      "******************************\n",
      "Source  : new jersey is never wet during november , and it is mild in august .\n",
      "Target  : new jersey est jamais humide au mois de novembre , et il est doux au mois d' août .\n",
      "Predict : new jersey est jamais humide au mois de novembre , et il est doux au mois d' août .\n",
      "\n",
      "==============================\n",
      "Epoch : 57/200 \n",
      "Training Loss : 0.063 \n",
      "stop_early : 19 \n",
      "coin_tossing_True_num : 0\n",
      "******************************\n",
      "Source  : china is warm during spring , and it is sometimes cold in february .\n",
      "Target  : chine est chaud au printemps , et il est parfois froid en février .\n",
      "Predict : chine est chaud au printemps , et il est parfois froid en février .\n",
      "\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "prob = 1e-3\n",
    "alpha = 1.2\n",
    "stop_early = 0\n",
    "coin_tossing_valid = [False for _ in range(0 , target_max_length)]\n",
    "for epoch_i in range(0 , epochs):\n",
    "    \n",
    "    # 在每進行一個epoch前，把每個batch的index先決定出來\n",
    "    batch_index = []\n",
    "    temp = []\n",
    "    count = 0 \n",
    "    while len(batch_index) <= 1059:  \n",
    "        temp.append(count)\n",
    "        count += 1\n",
    "        if len(temp) == batch_size:\n",
    "            batch_index.append(temp)\n",
    "            temp = []\n",
    "        if count == len(train_source):\n",
    "            count = 0\n",
    "    \n",
    "    # 一開始會出現[True , True , True , True , True , True , True , True]\n",
    "    # 隨著epoch的增長，逐漸開始出現[Faslse , Faslse , Faslse , Faslse , Faslse , Faslse , Faslse , Faslse]\n",
    "    # 在Decoder端的訓練，一開始t階段的輸入為t-1階段的期望輸出(Teacher Forcing)，到最後逐漸變為t階段的輸入為t-1階段的輸出(Sampling)\n",
    "    coin_tossing = np.random.choice(a = 2 , \n",
    "                                    size = target_max_length , \n",
    "                                    replace = True , \n",
    "                                    p = [prob , 1 - prob])\n",
    "    coin_tossing = coin_tossing.astype(bool)\n",
    "    if alpha * prob < 1  : prob = alpha * prob # p會隨著epoch增加越來越大\n",
    "    elif alpha * prob > 1: prob = 1.        \n",
    "\n",
    "    for batch_i in range(0 , 1059):\n",
    "        train_source_batch , train_target_batch =\\\n",
    "        train_source[batch_index[batch_i] , :] , train_target[batch_index[batch_i] , :] \n",
    "        train_target_length =\\\n",
    "        np.array(list(map(lambda x: len([y for y in x if y != 0]) , train_target_batch)))\n",
    "        \n",
    "        _ , training_loss =\\\n",
    "        sess.run([train_op , loss],\n",
    "                 feed_dict = {input_data : train_source_batch ,\n",
    "                              targets : train_target_batch ,\n",
    "                              target_sequence_length : train_target_length ,\n",
    "                              from_model_or_target : coin_tossing})\n",
    "   \n",
    "    predicting_logits_result =\\\n",
    "    sess.run(predicting_logits ,\n",
    "             feed_dict = {input_data : valid_source ,\n",
    "                          targets : valid_target ,\n",
    "                          from_model_or_target :coin_tossing_valid}) \n",
    "\n",
    "    print('=' *  30)\n",
    "    print('Epoch : {}/{} \\nTraining Loss : {:.3f} \\nstop_early : {} \\ncoin_tossing_True_num : {}'\n",
    "          .format(epoch_i , epochs ,\n",
    "                  training_loss , \n",
    "                  stop_early , \n",
    "                  coin_tossing.sum()))\n",
    "\n",
    "    index = np.random.randint(batch_size)\n",
    "\n",
    "    source_visualization = []\n",
    "    for i in valid_source[index]:\n",
    "        if source_int_to_letter[i] == '<PAD>': break\n",
    "        source_visualization.append(source_int_to_letter[i])\n",
    "    source_visualization = ' '.join(source_visualization)\n",
    "\n",
    "    target_visualization = []\n",
    "    for i in valid_target[index]:\n",
    "        if target_int_to_letter[i] == '<EOS>': break\n",
    "        target_visualization.append(target_int_to_letter[i])\n",
    "    target_visualization = ' '.join(target_visualization)\n",
    "\n",
    "    predict_visualization = []\n",
    "    for i in predicting_logits_result[index]:\n",
    "        if target_int_to_letter[i] == '<EOS>': break\n",
    "        predict_visualization.append(target_int_to_letter[i])\n",
    "    predict_visualization = ' '.join(predict_visualization)\n",
    "    \n",
    "    print('*' *  30)\n",
    "    print('Source  : {}'.format(source_visualization))\n",
    "    print('Target  : {}'.format(target_visualization))\n",
    "    print('Predict : {}\\n'.format(predict_visualization))\n",
    "\n",
    "    # 當coin_tossing全部為False的次數為20時即停止計算 \n",
    "    if coin_tossing.sum() == 0: stop_early += 1\n",
    "    if stop_early == 20: break\n",
    "\n",
    "# 保存模型\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess , 'trained_model/save_net')\n",
    "print('Model Trained and Saved')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c_BiEvGJ_NVn"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6KxNAPcm_NVu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from trained_model\\save_net\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "new_saver = tf.train.import_meta_graph(os.path.join('trained_model/save_net.meta'))\n",
    "new_saver.restore(sess, tf.train.latest_checkpoint(os.path.join('trained_model')))\n",
    "\n",
    "graph = tf.get_default_graph()\n",
    "input_data = graph.get_tensor_by_name('inputs:0')\n",
    "targets = graph.get_tensor_by_name('targets:0')\n",
    "logits = graph.get_tensor_by_name('predictions:0')\n",
    "from_model_or_target = graph.get_tensor_by_name('from_model_or_target:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 833,
     "status": "ok",
     "timestamp": 1598183038376,
     "user": {
      "displayName": "tomtom7852@kimo.com",
      "photoUrl": "",
      "userId": "13075538237903626622"
     },
     "user_tz": -480
    },
    "id": "7izlk2jZIKPT",
    "outputId": "274c74a7-f45b-43e2-d364-67c2338849d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "輸入的英文句子 : i dislike grapefruit , lemons , and peaches .\n",
      "google翻譯的法文句子 : je n'aime pamplemousses , les citrons et les pêches .\n",
      "model翻譯的法文句子  : je n'aime pamplemousses , les citrons et les pêches .\n"
     ]
    }
   ],
   "source": [
    "# 注意input_sentence的長度不能大於訓練語料庫的句子的最大長度\n",
    "input_sentence = 'i dislike grapefruit , lemons , and peaches .'\n",
    "test_source = []\n",
    "for letter in input_sentence.split(' '):\n",
    "    if letter in source_letter_to_int.keys():\n",
    "        test_source.append(source_letter_to_int[letter])\n",
    "    elif letter not in source_letter_to_int.values():\n",
    "        test_source.append(source_letter_to_int['<UNK>'])\n",
    "\n",
    "# 輸入的句子的長度是固定source_max_length，所以補source_letter_to_int['<PAD>']到長度為source_max_length\n",
    "while len(test_source) < source_max_length:\n",
    "    test_source.append(source_letter_to_int['<PAD>'])\n",
    "test_source = [test_source] * batch_size\n",
    "\n",
    "# test_target輸入的值可以隨便選，只要長度為target_max_length即可\n",
    "# 雖然這一項對於在inference時不需要，但是還是要把這一項丟進 NN\n",
    "test_target = [0 for _ in range(0 , target_max_length)] \n",
    "test_target = [test_target] * batch_size\n",
    "\n",
    "test_source = np.array(test_source)\n",
    "test_target = np.array(test_target)\n",
    "coin_tossing = [False for _ in range(0 , target_max_length)]\n",
    "answer = sess.run(logits, feed_dict = {input_data : test_source ,\n",
    "                                       targets : test_target ,\n",
    "                                       from_model_or_target : coin_tossing})\n",
    "\n",
    "answer = answer[0 , :]\n",
    "answer_to_letter = []\n",
    "for num in answer:\n",
    "    if target_int_to_letter[num] == '<EOS>': break\n",
    "    answer_to_letter.append(target_int_to_letter[num])\n",
    "\n",
    "print('輸入的英文句子 : {}'.format(input_sentence))  \n",
    "print('google翻譯的法文句子 : {}'.format(\"je n'aime pamplemousses , les citrons et les pêches .\"))  \n",
    "print('model翻譯的法文句子  : {}'.format(' '.join(answer_to_letter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1040,
     "status": "ok",
     "timestamp": 1598183222961,
     "user": {
      "displayName": "tomtom7852@kimo.com",
      "photoUrl": "",
      "userId": "13075538237903626622"
     },
     "user_tz": -480
    },
    "id": "GhiBEFyiIeyT",
    "outputId": "41cea045-e1db-4e3b-943c-0a98bdf15fa3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "輸入的英文句子 : california is never cold during december , but it is usually warm in may .\n",
      "google翻譯的法文句子 : la californie n'est jamais froide en décembre, mais il fait généralement chaud en mai.\n",
      "model翻譯的法文句子  : californie ne fait jamais froid en décembre , mais il est habituellement chaud en mai .\n"
     ]
    }
   ],
   "source": [
    "# 注意input_sentence的長度不能大於訓練語料庫的句子的最大長度\n",
    "input_sentence = 'california is never cold during december , but it is usually warm in may .'\n",
    "test_source = []\n",
    "for letter in input_sentence.split(' '):\n",
    "    if letter in source_letter_to_int.keys():\n",
    "        test_source.append(source_letter_to_int[letter])\n",
    "    elif letter not in source_letter_to_int.values():\n",
    "        test_source.append(source_letter_to_int['<UNK>'])\n",
    "\n",
    "# 輸入的句子的長度是固定source_max_length，所以補source_letter_to_int['<PAD>']到長度為source_max_length\n",
    "while len(test_source) < source_max_length:\n",
    "    test_source.append(source_letter_to_int['<PAD>'])\n",
    "test_source = [test_source] * batch_size\n",
    "\n",
    "# test_target輸入的值可以隨便選，只要長度為target_max_length即可\n",
    "# 雖然這一項對於在inference時不需要，但是還是要把這一項丟進 NN\n",
    "test_target = [0 for _ in range(0 , target_max_length)] \n",
    "test_target = [test_target] * batch_size\n",
    "\n",
    "test_source = np.array(test_source)\n",
    "test_target = np.array(test_target)\n",
    "coin_tossing = [False for _ in range(0 , target_max_length)]\n",
    "answer = sess.run(logits, feed_dict = {input_data : test_source ,\n",
    "                                       targets : test_target ,\n",
    "                                       from_model_or_target : coin_tossing})\n",
    "\n",
    "answer = answer[0 , :]\n",
    "answer_to_letter = []\n",
    "for num in answer:\n",
    "    if target_int_to_letter[num] == '<EOS>': break\n",
    "    answer_to_letter.append(target_int_to_letter[num])\n",
    "\n",
    "print('輸入的英文句子 : {}'.format(input_sentence))  \n",
    "print('google翻譯的法文句子 : {}'.format(\"la californie n'est jamais froide en décembre, mais il fait généralement chaud en mai.\"))  \n",
    "print('model翻譯的法文句子  : {}'.format(' '.join(answer_to_letter)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "2_scheduled_sampling_attension_不使用helper指令.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
