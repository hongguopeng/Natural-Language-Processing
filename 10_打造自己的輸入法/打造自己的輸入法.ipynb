{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 請解壓縮data.rar，取得本程式之數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import codecs\n",
    "from modules import embed , normalize , conv1d , conv1d_banks , gru , prenet , highwaynet\n",
    "from hyperparams import Hyperparams as hp\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from xpinyin import Pinyin \n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 數據預處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean(text):\n",
    "#     if re.search('[A-Za-z0-9]', text) is None: # 只要有英文或數字的sent，全部略過\n",
    "#         text = re.sub('[^\\u4e00-\\u9fa5。，！？]' , '' , text) # 只要是\"漢字。，！？\"以外的詞換都換成''\n",
    "#         text = text.replace(' ' , '')\n",
    "#         return text\n",
    "#     else:    \n",
    "#         return ''\n",
    "\n",
    "# def align(sent):\n",
    "    \n",
    "#     pinyin = Pinyin()\n",
    "#     pnyns = pinyin.get_pinyin(sent , ' ').split()\n",
    "    \n",
    "#     # 所有不足拼音的漢字都要加上'_'直到長度等於該漢字對應的拼音\n",
    "#     # 漢字 : [大_] [巴_] [的_] [门__] [能__]  [将____] [人__] [夹__] [死_]\n",
    "#     # 拼音 : [da]  [ba]  [de]  [men]  [nen]  [gjiang] [ren]  [jia]  [si]\n",
    "#     hanzis = []\n",
    "#     for i , word in enumerate(sent):\n",
    "#         while len(word) < len(pnyns[i]):\n",
    "#             word += '_'\n",
    "#             if len(word) == len(pnyns[i]):\n",
    "#                 break\n",
    "#         hanzis.append(word)\n",
    "    \n",
    "#     pnyns = ''.join(pnyns)\n",
    "#     hanzis = ''.join(hanzis)\n",
    "    \n",
    "#     return pnyns , hanzis\n",
    "\n",
    "# fout = codecs.open('data/zh.tsv' , 'w' , 'utf-8')\n",
    "# fin = codecs.open('data/zho_news_2007-2009_1M-sentences.txt' , 'r' , 'utf-8')\n",
    "# lines = fin.readlines()  \n",
    "# for i , line in enumerate(lines):         \n",
    "#     idx , sent = line.strip().split('\\t')\n",
    "#     sent = clean(sent)\n",
    "#     if len(sent) > 0:\n",
    "#         try:\n",
    "#             pnyns , hanzis = align(sent)\n",
    "#             fout.write('{}\\t{}\\t{}\\n'.format(idx , pnyns , hanzis))\n",
    "#         except:\n",
    "#             print('這一句有問題 : {}'.format(line))\n",
    "#             pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讀取數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = open('data/zh.tsv', 'r' , encoding = 'utf-8')\n",
    "lines = fin.readlines()\n",
    "\n",
    "source_data , target_data = [] , []\n",
    "for line in lines:\n",
    "    try:\n",
    "        _  , pnyn_sents , hanzi_sents = line.strip().split('\\t')\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    # 我很無聊，他也很無聊。大家都很無聊。 \n",
    "    # 我很無聊，|他也很無聊。|大家都很無聊。|\n",
    "    pnyn_sents = re.sub('(?<=[。，！？])' , r'|' , pnyn_sents).split('|')  # 在。，！？後面加上|，比較好從這些標點符號的地方將整個句子分開\n",
    "    hanzi_sents = re.sub('(?<=[。，！？])' , r'|' , hanzi_sents).split('|')  # 在。，！？後面加上|，比較好從這些標點符號的地方將整個句子分開\n",
    "\n",
    "    for pnyn_sent , hanzi_sent in zip(pnyn_sents , hanzi_sents):   \n",
    "        assert len(pnyn_sent) == len(hanzi_sent)\n",
    "        if 10 < len(pnyn_sent) <= 50: # 考慮長度在[10 , 50)句子     \n",
    "            x = [pnyn for pnyn in pnyn_sent]\n",
    "            y = [hanzi for hanzi in hanzi_sent] \n",
    "            if len(source_data) < 80000: # 只收集80000筆資料\n",
    "                source_data.append(x)\n",
    "                target_data.append(y)           \n",
    "    if len(source_data) == 80000: # 收集80000筆資料，停止迴圈\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立拼音的詞庫\n",
    "pnyns = 'EUabcdefghijklmnopqrstuvwxyz0123456789。，！？' # E: Empty , U: Unknown\n",
    "pnyn_to_int = {}\n",
    "for idx , pnyn in enumerate(pnyns):\n",
    "    pnyn_to_int[pnyn] = idx\n",
    "int_to_pnyn = dict(zip(pnyn_to_int.values() , pnyn_to_int.keys()))   \n",
    "\n",
    "# 建立漢字的詞庫\n",
    "counter = {}\n",
    "for hanzi_sent in target_data:\n",
    "    for word in hanzi_sent:\n",
    "        if word in counter.keys():\n",
    "            counter[word] += 1\n",
    "        elif word not in counter.keys():   \n",
    "            counter[word] = 1\n",
    "hanzis = [hanzi for hanzi , cnt in counter.items() if cnt > 5] # 把詞頻小於5的全部刪除\n",
    "hanzis.remove('_')\n",
    "hanzis = ['E' , 'U' , '_' ] + hanzis  # 0 : Empty->E , 1 : Unknown->U , 2 : Blank->B\n",
    "hanzi_to_int = {hanzi : int_ for int_ , hanzi in enumerate(hanzis)}\n",
    "int_to_hanzi = dict(zip(hanzi_to_int.values() , hanzi_to_int.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 決定seq的最大長度 \n",
    "max_seq_len = max([len(x) for x in source_data])\n",
    "\n",
    "# 將source_temp與target_temp中的字母全部換成index     \n",
    "source_pad , target_pad = [] , []\n",
    "for x , y in zip(source_data , target_data):\n",
    "    source_temp , target_temp = [] , []    \n",
    "    for x_i , y_i in zip(x , y):\n",
    "        if x_i not in pnyn_to_int.keys(): source_temp.append(1) # 1 -> unknown\n",
    "        else: source_temp.append(pnyn_to_int[x_i])\n",
    "        if y_i not in hanzi_to_int.keys(): target_temp.append(1) # 1 -> unknown\n",
    "        else: target_temp.append(hanzi_to_int[y_i])\n",
    "    \n",
    "    # 補0補到長度為max_seq_len\n",
    "    while len(source_temp) < max_seq_len:\n",
    "        source_temp.append(0)  # 0 -> empty\n",
    "        target_temp.append(0)  # 0 -> empty\n",
    "                \n",
    "    source_pad.append(source_temp)  \n",
    "    target_pad.append(target_temp)  \n",
    "            \n",
    "source_pad = np.array(source_pad).astype(int) \n",
    "target_pad = np.array(target_pad).astype(int)  \n",
    "\n",
    "# 將數據分為訓練集與測試集\n",
    "(trainX , testX , trainY , testY) = train_test_split(source_pad ,\n",
    "                                                     target_pad , \n",
    "                                                     test_size = 0.01 , \n",
    "                                                     random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = tf.placeholder(tf.int32 , [None , max_seq_len] , name = 'source')\n",
    "target = tf.placeholder(tf.int32 , [None , max_seq_len] , name = 'target')\n",
    "target_onehot = tf.one_hot(tf.reshape(target , [-1]) , depth = len(hanzi_to_int))\n",
    "on_train = tf.placeholder(tf.bool) # train/test selector for batch normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-bda504476c92>:55: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Character Embedding for x\n",
    "enc = embed(source , len(pnyn_to_int) , hp.embed_size , scope = 'emb_x')\n",
    "\n",
    "# Encoder pre-net\n",
    "prenet_out = prenet(enc,\n",
    "                    num_units = [hp.embed_size , hp.embed_size // 2],\n",
    "                    is_training = True) \n",
    "\n",
    "## Conv1D bank => 考慮不同捲積長度，長度越長代表看到的上下文越多，長度越短代表看到的上下文越少，最後可分別將不同捲積長度的輸出堆疊起來\n",
    "enc , ema_0 = conv1d_banks(prenet_out ,\n",
    "                           on_train ,\n",
    "                           K = hp.encoder_num_banks ,\n",
    "                           num_units = hp.embed_size // 2)  \n",
    "\n",
    "## Max pooling\n",
    "enc = tf.layers.max_pooling1d(enc , 2 , 1 , padding = 'same') \n",
    "\n",
    "## Conv1D projections\n",
    "enc = conv1d(enc , hp.embed_size // 2 , 5, scope = 'conv1d_1')\n",
    "\n",
    "enc , ema_1 = normalize(enc , \n",
    "                        on_train , \n",
    "                        type = hp.norm_type , \n",
    "                        activation_fn = tf.nn.relu , \n",
    "                        scope = 'norm1')\n",
    "\n",
    "enc = conv1d(enc , hp.embed_size // 2 , 5 , scope='conv1d_2')\n",
    "  \n",
    "enc , ema_2 = normalize(enc ,\n",
    "                        on_train , \n",
    "                        type = hp.norm_type , \n",
    "                        activation_fn = None ,\n",
    "                        scope = 'norm2')\n",
    "\n",
    "enc += prenet_out  \n",
    "\n",
    "## Highway Nets\n",
    "for i in range(hp.num_highwaynet_blocks):\n",
    "    enc = highwaynet(enc , num_units = hp.embed_size // 2,\n",
    "                     scope = 'highwaynet_{}'.format(i))  \n",
    "\n",
    "## Bidirectional GRU\n",
    "enc = gru(enc , hp.embed_size // 2 , True , scope = 'gru1')  \n",
    "\n",
    "## Readout\n",
    "outputs = tf.layers.dense(enc , len(hanzi_to_int) , use_bias = False)\n",
    "logits = tf.reshape(outputs , [-1 , len(hanzi_to_int)])\n",
    "\n",
    "## predicting_logits只是要觀察結果，不列入計算\n",
    "predicting_logits = tf.nn.softmax(logits)\n",
    "predicting_logits = tf.argmax(predicting_logits , axis = 1)\n",
    "predicting_logits = tf.reshape(predicting_logits , [-1 , max_seq_len])\n",
    "\n",
    "## compute loss\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(labels = target_onehot , logits = logits)\n",
    "total_loss = tf.reduce_mean(loss)\n",
    "\n",
    "## optimizer\n",
    "update_ema = tf.group([ema_0 , ema_1 , ema_2])\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = hp.lr)\n",
    "train_op = optimizer.minimize(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Epoch : 0/30\n",
      "Batch : 0/400\n",
      "Training Loss : 8.098\n",
      "source_value : yidanfashengjiufen，\n",
      "target_value : 一_旦__发_生____纠__纷__，\n",
      "predicting_value : 胎坞坞冻果果果果果果果果果果果果穆果果果药药果果果果果果药果果果果药果果果果果果果果果果果果果果果村\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "num_batch = math.ceil(len(source_pad)/ hp.batch_size)\n",
    "train_loss_mean_record = []\n",
    "val_loss_record = []\n",
    "for epoch_i in range(0 , hp.num_epochs): \n",
    "    # shuffle index\n",
    "    index = np.arange(len(trainX))\n",
    "    np.random.shuffle(index)\n",
    "    trainX = trainX[index]\n",
    "    trainY = trainY[index]\n",
    "     \n",
    "    # 在每進行一個epoch前，把每個batch的index先決定出來\n",
    "    batch_index = []\n",
    "    temp = []\n",
    "    count = 0 # 隨機決定index的開頭 \n",
    "    while len(batch_index) <= num_batch:  # 1個batch裡只有num_batch筆資料\n",
    "        temp.append(count)\n",
    "        count += 1\n",
    "        if len(temp) == hp.batch_size:\n",
    "            batch_index.append(temp)\n",
    "            temp = []\n",
    "        if count == len(trainX):\n",
    "            count = 0\n",
    "    \n",
    "    train_loss_temp = []        \n",
    "    for batch_i in range(0 , num_batch):\n",
    "        source_pad_batch , target_pad_batch =\\\n",
    "        trainX[batch_index[batch_i] , :] , trainY[batch_index[batch_i] , :] \n",
    "        \n",
    "        feed_dict = {source : source_pad_batch , \n",
    "                     target : target_pad_batch , \n",
    "                     on_train : True}\n",
    "        \n",
    "        _ , _ , training_loss , predicting_value = sess.run([train_op , update_ema , total_loss , predicting_logits] , feed_dict)\n",
    "        train_loss_temp.append(training_loss) \n",
    "\n",
    "        idx = random.randint(0 , hp.batch_size - 1) # 決定想要查看的index\n",
    "        source_value = source_pad_batch[idx , :]\n",
    "        target_value = target_pad_batch[idx , :]\n",
    "        predicting_value = predicting_value[idx , :]\n",
    "\n",
    "        source_result = ''\n",
    "        for int_source in source_value:\n",
    "            if int_to_pnyn[int_source] == 'E' : break\n",
    "            source_result += int_to_pnyn[int_source]\n",
    "\n",
    "        target_result = ''\n",
    "        for int_target in target_value:\n",
    "            if int_to_hanzi[int_target] == 'E' : break\n",
    "            target_result += int_to_hanzi[int_target]\n",
    "\n",
    "        predicting_result = ''\n",
    "        for int_predicting in predicting_value:\n",
    "            if int_to_hanzi[int_predicting] == 'E' : break\n",
    "            predicting_result += int_to_hanzi[int_predicting]\n",
    "\n",
    "        if batch_i % 200 == 0:\n",
    "            print('=' * 70)\n",
    "            print('Epoch : {}/{}'.format(epoch_i , hp.num_epochs))\n",
    "            print('Batch : {}/{}'.format(batch_i , num_batch))\n",
    "            print('Training Loss : {:.3f}'.format(training_loss))\n",
    "            print('source_value : {}'.format(source_result))\n",
    "            print('target_value : {}'.format(target_result))\n",
    "            print('predicting_value : {}\\n'.format(predicting_result))\n",
    "    \n",
    "    feed_dict = {source : testX , target : testY , on_train : False}\n",
    "    val_loss = sess.run(total_loss , feed_dict)    \n",
    "    print('*' * 30) \n",
    "    print('Epoch : {}/{}'.format(epoch_i , hp.num_epochs))               \n",
    "    print('train_loss_batch_mean : {:.2f}'.format(np.array(train_loss_temp).mean()))\n",
    "    print('val_loss : {:.2f}'.format(val_loss))\n",
    "    print('*' * 30 , '\\n')  \n",
    "    train_loss_mean_record.append(np.array(train_loss_temp).mean())\n",
    "    val_loss_record.append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model for checkpoint\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess , './model/' , global_step = epoch_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 看一下testX的預測結果\n",
    "feed_dict = {source : testX , on_train : False}\n",
    "predicting_value_valid = sess.run(predicting_logits , feed_dict)\n",
    "\n",
    "for source_value , target_value , predicting_value in zip(testX , testY , predicting_value_valid):\n",
    "\n",
    "    source_result_valid = ''\n",
    "    for int_source in source_value:\n",
    "        if int_to_pnyn[int_source] == 'E' : break\n",
    "        source_result_valid += int_to_pnyn[int_source]\n",
    "\n",
    "    target_result_valid = ''\n",
    "    for int_target in target_value:\n",
    "        if int_to_hanzi[int_target] == 'E' : break\n",
    "        target_result_valid += int_to_hanzi[int_target]\n",
    "\n",
    "    predicting_result_valid = ''\n",
    "    for int_predicting in predicting_value:\n",
    "        if int_to_hanzi[int_predicting] == 'E' : break\n",
    "        predicting_result_valid += int_to_hanzi[int_predicting]\n",
    "             \n",
    "    print('=' * 30)\n",
    "    print('source_value : {}'.format(source_result_valid))\n",
    "    print('target_value : {}'.format(target_result_valid))\n",
    "    print('predicting_value : {}'.format(predicting_result_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
