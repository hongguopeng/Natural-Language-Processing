{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用CNN對中文文本進行正面與負面語意的分類\n",
    "CNN只是一種提取特徵的工具，只要讓資料整理成類似於影像的格式，照樣可以丟進CNN<br />\n",
    "而最後在測試集上的準確率可以達到90%以上!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 讀取數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_examples = open('chinese/rt-polarity.pos' , 'r' , encoding = 'utf-8').readlines()\n",
    "positive_examples = [s.strip() for s in positive_examples] \n",
    "negative_examples = open('chinese/rt-polarity.neg' , 'r' , encoding = 'utf-8').readlines()\n",
    "negative_examples = [s.strip() for s in negative_examples] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 數據預處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = positive_examples + negative_examples\n",
    "# 把sentence中的空白刪除\n",
    "sentences = [sentence.split(' ') for sentence in sentences]\n",
    "\n",
    "# 生成標籤\n",
    "positive_labels = [[0 , 1] for _ in positive_examples]\n",
    "negative_labels = [[1 , 0] for _ in negative_examples]\n",
    "labels = np.concatenate([positive_labels , negative_labels] , axis = 0)\n",
    "\n",
    "sequence_length = 50\n",
    "sentences_padding = []\n",
    "for i in range(0 , len(sentences)):\n",
    "    sentence = sentences[i]\n",
    "    if sequence_length > len(sentence):\n",
    "        # 不足sequence_length的sentence，就直接補'PAD'，補到長度為sequence_length\n",
    "        while True:\n",
    "            sentence += ['PAD']\n",
    "            if len(sentence) == sequence_length:\n",
    "                break\n",
    "        sentences_padding.append(sentence)\n",
    "    else:\n",
    "        sentences_padding.append(sentence[:sequence_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_counts包含了每個字對應的頻率\n",
    "word_counts = {}\n",
    "for sent in sentences_padding:\n",
    "    for word in sent:\n",
    "        if word not in word_counts.keys():\n",
    "            word_counts[word] = 1\n",
    "        else:\n",
    "            word_counts[word] += 1\n",
    "word_counts = sorted(word_counts.items() , key = lambda item : item[1] , reverse = True)\n",
    "\n",
    "# 取前多少個常用字，可以將len(word_counts)換成其他數目\n",
    "words = [i[0] for i in word_counts]\n",
    "words = words[:len(words) + 1]\n",
    "\n",
    "# 每個字映射為一個數字ID\n",
    "word_to_int = {}\n",
    "for idx , word in enumerate(words):\n",
    "    word_to_int[word] = idx\n",
    "    \n",
    "x = []\n",
    "for sentence in sentences_padding:\n",
    "    # 把sentence的每個詞拿出來，去找出word_to_int所對應的index，組合起來當作這個名字的向量\n",
    "    temp = [word_to_int[word] for word in sentence]\n",
    "    x.append(temp)\n",
    "x = np.array(x)\n",
    "y = np.array(labels)   \n",
    "\n",
    "# 切分數據集\n",
    "train_x , test_x , train_y , test_y = \\\n",
    "train_test_split(x , y , test_size = 0.1 , random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立神經網路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter\n",
    "input_size = sequence_length\n",
    "num_classes = 2 # 標籤總共2種類別\n",
    "batch_size = 30\n",
    "num_batch = len(train_x) // batch_size\n",
    "vocabulary_size = len(word_to_int)\n",
    "embedding_size = 256\n",
    "num_filters = 512\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None , input_size])\n",
    "Y = tf.placeholder(tf.float32, [None , num_classes])\n",
    "dropout_keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# embedding layer\n",
    "vocabulary_size = len(word_to_int)\n",
    "with tf.variable_scope('embedding'):\n",
    "    initializer = tf.contrib.layers.xavier_initializer()\n",
    "    W = tf.Variable(initializer([vocabulary_size , embedding_size]) , name = 'embedding')\n",
    "    embedded_chars = tf.nn.embedding_lookup(W , X)\n",
    "    # 為了要丟進CNN，所以最後加了一個維度，這樣就類似於影像了\n",
    "    embedded_chars_expanded = tf.expand_dims(embedded_chars , 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](工作流程.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "依據考慮上下文的長度的不同，可以用不同長度的filter<br />\n",
    "但最後的output的維度會不同，此時可以透過max pooling做調整，將輸出統一成同一個維度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cnn_filter.png\" style=\"width:750px;height:400px;float:middle\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step_0 → 輸入CNN之前的維度 : (?, 50, 256, 1)\n",
      "\n",
      "step_1-1 → 以第1種filtert處理之後的維度 : (?, 48, 1, 512)\n",
      "\n",
      "step_1-2 → 以第1次pooling之後的維度 : (?, 1, 1, 512)\n",
      "\n",
      "step_2-1 → 以第2種filtert處理之後的維度 : (?, 47, 1, 512)\n",
      "\n",
      "step_2-2 → 以第2次pooling之後的維度 : (?, 1, 1, 512)\n",
      "\n",
      "step_3-1 → 以第3種filtert處理之後的維度 : (?, 46, 1, 512)\n",
      "\n",
      "step_3-2 → 以第3次pooling之後的維度 : (?, 1, 1, 512)\n",
      "\n",
      "step_4 → 將(pooled_1 , pooled_2 , pooled_3)拼接之後的維度 : (?, 1, 1, 1536)\n"
     ]
    }
   ],
   "source": [
    "print('step_0 → 輸入CNN之前的維度 : {}\\n'.format(embedded_chars_expanded.shape))\n",
    "\n",
    "# convolution + maxpool layer\n",
    "pooled_outputs = []\n",
    "filter_sizes = [3 , 4 , 5] # 總共有3種filter\n",
    "with tf.variable_scope('conv_layer_{}'.format(filter_sizes[0])):\n",
    "    initializer = tf.contrib.layers.xavier_initializer()\n",
    "    filter_shape = [filter_sizes[0] , embedding_size , 1 , num_filters]\n",
    "    W_conv1 = tf.Variable(initializer(filter_shape) , name = 'weight')\n",
    "    b_conv1 = tf.Variable(tf.constant(0.0001 , shape = [num_filters]) , name = 'bias')\n",
    "    conv_1 = tf.nn.conv2d(embedded_chars_expanded ,\n",
    "                          W_conv1 ,\n",
    "                          strides = [1 , 1 , 1 , 1] , \n",
    "                          padding = 'VALID')\n",
    "    h_1 = tf.nn.relu(conv_1 + b_conv1)\n",
    "    print('step_1-1 → 以第1種filtert處理之後的維度 : {}\\n'.format(h_1.shape))\n",
    "   \n",
    "    pooled_1 = tf.nn.max_pool(h_1 , \n",
    "                              ksize = [1 , input_size - filter_sizes[0] + 1 , 1 , 1] , \n",
    "                              strides = [1 , 1 , 1 , 1] , \n",
    "                              padding = 'VALID')\n",
    "    print('step_1-2 → 以第1次pooling之後的維度 : {}\\n'.format(pooled_1.shape))\n",
    "    pooled_outputs.append(pooled_1)\n",
    "\n",
    "    \n",
    "with tf.variable_scope('conv_layer_{}'.format(filter_sizes[1])):\n",
    "    initializer = tf.contrib.layers.xavier_initializer()\n",
    "    filter_shape = [filter_sizes[1] , embedding_size , 1 , num_filters]\n",
    "    W_conv2 = tf.Variable(initializer(filter_shape) , name = 'weight')\n",
    "    b_conv2 = tf.Variable(tf.constant(0.0001 , shape = [num_filters]) , name = 'bias')\n",
    "    conv_2 = tf.nn.conv2d(embedded_chars_expanded , \n",
    "                          W_conv2 , \n",
    "                          strides = [1 , 1 , 1 , 1] , \n",
    "                          padding = 'VALID')\n",
    "    h_2 = tf.nn.relu(conv_2 + b_conv2)\n",
    "    print('step_2-1 → 以第2種filtert處理之後的維度 : {}\\n'.format(h_2.shape))\n",
    "    \n",
    "    pooled_2 = tf.nn.max_pool(h_2 , \n",
    "                              ksize = [1 , input_size - filter_sizes[1] + 1 , 1 , 1] , \n",
    "                              strides = [1 , 1 , 1 , 1] , \n",
    "                              padding = 'VALID')\n",
    "    print('step_2-2 → 以第2次pooling之後的維度 : {}\\n'.format(pooled_2.shape))\n",
    "    pooled_outputs.append(pooled_2)\n",
    "\n",
    "    \n",
    "with tf.variable_scope('conv_layer_{}'.format(filter_sizes[2])):\n",
    "    initializer = tf.contrib.layers.xavier_initializer()\n",
    "    filter_shape = [filter_sizes[2] , embedding_size , 1 , num_filters]\n",
    "    W_conv3 = tf.Variable(initializer(filter_shape) , name = 'weight')\n",
    "    b_conv3 = tf.Variable(tf.constant(0.0001 , shape = [num_filters]) , name = 'bias')\n",
    "    conv_3 = tf.nn.conv2d(embedded_chars_expanded , \n",
    "                          W_conv3 , \n",
    "                          strides = [1 , 1 , 1 , 1] , \n",
    "                          padding = 'VALID')\n",
    "    h_3 = tf.nn.relu(conv_3 + b_conv3)\n",
    "    print('step_3-1 → 以第3種filtert處理之後的維度 : {}\\n'.format(h_3.shape))\n",
    "    \n",
    "    pooled_3 = tf.nn.max_pool(h_3 , \n",
    "                              ksize = [1 , input_size - filter_sizes[2] + 1 , 1 , 1] , \n",
    "                              strides = [1 , 1 , 1 , 1] , \n",
    "                              padding = 'VALID')\n",
    "    print('step_3-2 → 以第3次pooling之後的維度 : {}\\n'.format(pooled_3.shape))\n",
    "    pooled_outputs.append(pooled_3)\n",
    "\n",
    "    \n",
    "num_filters_total = num_filters * len(filter_sizes)\n",
    "h_pool = tf.concat(pooled_outputs , axis = 3)\n",
    "print('step_4 → 將(pooled_1 , pooled_2 , pooled_3)拼接之後的維度 : {}'.format(h_pool.shape))\n",
    "h_pool_flat = tf.reshape(h_pool , [-1 , num_filters_total])\n",
    "\n",
    "# dropout\n",
    "with tf.variable_scope('dropout'):\n",
    "    h_drop = tf.nn.dropout(h_pool_flat , dropout_keep_prob)\n",
    "    \n",
    "# output\n",
    "with tf.variable_scope('output'):\n",
    "    W_flat = tf.get_variable(shape = [num_filters_total, num_classes] , \n",
    "                             initializer = tf.contrib.layers.xavier_initializer() , \n",
    "                             name = 'weight')\n",
    "    b_flat = tf.Variable(tf.constant(0.1 , shape = [num_classes]) , name = 'bias')\n",
    "    output = tf.nn.xw_plus_b(h_drop , W_flat , b_flat)\n",
    "    output = tf.nn.softmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = tf.equal(tf.cast(tf.greater_equal(output , 0.5) , tf.int32) , tf.cast(Y , tf.int32))\n",
    "accuracy = tf.reduce_mean(tf.reduce_min(tf.cast(correct , tf.float32) , 1))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "cross_entropy_temp = -tf.reduce_sum(Y * tf.log(output + 1e-9) , axis = 1)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy_temp)\n",
    "grads_and_vars = optimizer.compute_gradients(cross_entropy)\n",
    "train_op = optimizer.apply_gradients(grads_and_vars)\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 開始訓練神經網路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "epoch_i : 0\n",
      "batch_i : 0\n",
      "train_loss : 0.69\n",
      "train_accuracy : 36.67%\n",
      "\n",
      "==============================\n",
      "epoch_i : 0\n",
      "batch_i : 700\n",
      "train_loss : 0.34\n",
      "train_accuracy : 86.67%\n",
      "\n",
      "==============================\n",
      "epoch_i : 0\n",
      "batch_i : 1400\n",
      "train_loss : 0.38\n",
      "train_accuracy : 90.00%\n",
      "\n",
      "******************************\n",
      "epoch_i : 0\n",
      "test_loss : 0.28\n",
      "test_accuracy : 89.21%\n",
      "****************************** \n",
      "\n",
      "==============================\n",
      "epoch_i : 1\n",
      "batch_i : 0\n",
      "train_loss : 0.14\n",
      "train_accuracy : 100.00%\n",
      "\n",
      "==============================\n",
      "epoch_i : 1\n",
      "batch_i : 700\n",
      "train_loss : 0.19\n",
      "train_accuracy : 96.67%\n",
      "\n",
      "==============================\n",
      "epoch_i : 1\n",
      "batch_i : 1400\n",
      "train_loss : 0.37\n",
      "train_accuracy : 90.00%\n",
      "\n",
      "******************************\n",
      "epoch_i : 1\n",
      "test_loss : 0.26\n",
      "test_accuracy : 90.86%\n",
      "****************************** \n",
      "\n",
      "==============================\n",
      "epoch_i : 2\n",
      "batch_i : 0\n",
      "train_loss : 0.09\n",
      "train_accuracy : 100.00%\n",
      "\n",
      "==============================\n",
      "epoch_i : 2\n",
      "batch_i : 700\n",
      "train_loss : 0.08\n",
      "train_accuracy : 96.67%\n",
      "\n",
      "==============================\n",
      "epoch_i : 2\n",
      "batch_i : 1400\n",
      "train_loss : 0.29\n",
      "train_accuracy : 93.33%\n",
      "\n",
      "******************************\n",
      "epoch_i : 2\n",
      "test_loss : 0.28\n",
      "test_accuracy : 90.91%\n",
      "****************************** \n",
      "\n",
      "==============================\n",
      "epoch_i : 3\n",
      "batch_i : 0\n",
      "train_loss : 0.06\n",
      "train_accuracy : 100.00%\n",
      "\n",
      "==============================\n",
      "epoch_i : 3\n",
      "batch_i : 700\n",
      "train_loss : 0.04\n",
      "train_accuracy : 100.00%\n",
      "\n",
      "==============================\n",
      "epoch_i : 3\n",
      "batch_i : 1400\n",
      "train_loss : 0.15\n",
      "train_accuracy : 93.33%\n",
      "\n",
      "******************************\n",
      "epoch_i : 3\n",
      "test_loss : 0.31\n",
      "test_accuracy : 91.04%\n",
      "****************************** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# minibatch data index\n",
    "epochs = 4\n",
    "step = (math.ceil(len(train_x) / batch_size)) * batch_size\n",
    "temp = []\n",
    "j = 0\n",
    "index = []\n",
    "for ii in range(0 , step):\n",
    "    j = j + 1\n",
    "    if j > len(train_x):\n",
    "        j = j - (len(train_x))\n",
    "    temp.append(j)\n",
    "    if len(temp) == batch_size:\n",
    "        index.append(temp)\n",
    "        temp = []\n",
    "index = list(np.array(index) - 1)\n",
    "\n",
    "for epoch_i in range(0 , epochs):\n",
    "    for batch_i in range(0 , num_batch):\n",
    "        batch_x = train_x[index[batch_i] , :]\n",
    "        batch_y = train_y[index[batch_i] , :]\n",
    "\n",
    "        feed_dict = {X : batch_x , Y : batch_y , dropout_keep_prob : 0.9}\n",
    "        _ , train_loss , train_acc = sess.run([train_op , cross_entropy , accuracy] , feed_dict)\n",
    "        \n",
    "        if batch_i % 700 == 0:\n",
    "            print('=' * 30)\n",
    "            print('epoch_i : {}'.format(epoch_i))\n",
    "            print('batch_i : {}'.format(batch_i))\n",
    "            print('train_loss : {:.2f}'.format(train_loss))\n",
    "            print('train_accuracy : {:.2%}\\n'.format(train_acc))\n",
    "\n",
    "    feed_dict = {X : test_x , Y : test_y , dropout_keep_prob : 1}\n",
    "    test_loss , test_acc = sess.run([cross_entropy , accuracy] , feed_dict)\n",
    "\n",
    "    print('*' * 30)\n",
    "    print('epoch_i : {}'.format(epoch_i))\n",
    "    print('test_loss : {:.2f}'.format(test_loss))\n",
    "    print('test_accuracy : {:.2%}'.format(test_acc))\n",
    "    print('*' * 30 , '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
